{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3993bc0",
   "metadata": {},
   "source": [
    "## DL 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656510ae",
   "metadata": {},
   "source": [
    "### 1.\tWrite the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa15e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the input data\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "# Define the weights and bias\n",
    "w = np.array([0.2, 0.4, 0.1])\n",
    "b = 0.3\n",
    "\n",
    "# Define the neuron output function (here, a simple dot product with a bias term)\n",
    "output = np.dot(x, w) + b\n",
    "\n",
    "print(output)  # This will print the output of the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f1ec2",
   "metadata": {},
   "source": [
    "### 2.\tWrite the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28219c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the input data\n",
    "x = np.array([-1, 2, -3, 4, -5])\n",
    "\n",
    "# Define the ReLU function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Apply ReLU to the input data\n",
    "output = relu(x)\n",
    "\n",
    "print(output)  # This will print the ReLU output of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6686164",
   "metadata": {},
   "source": [
    "### 3.\tWrite the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65660b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\kmeen\\appdata\\roaming\\python\\python37\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kmeen\\appdata\\roaming\\python\\python37\\site-packages (from torch) (4.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e33ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\kmeen\\anaconda3\\lib\\site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-23.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input and weight tensors\n",
    "input_tensor = torch.randn(3, 4)\n",
    "weight_tensor = torch.randn(4, 5)\n",
    "\n",
    "# Perform matrix multiplication with PyTorch's built-in function\n",
    "output_tensor = torch.matmul(input_tensor, weight_tensor)\n",
    "\n",
    "print(output_tensor)  # This will print the output of the dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee77380",
   "metadata": {},
   "source": [
    "### 4.\tWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and weight matrices as lists of lists\n",
    "input_matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "weight_matrix = [[0.1, 0.2, 0.3, 0.4, 0.5], [0.6, 0.7, 0.8, 0.9, 1.0], [1.1, 1.2, 1.3, 1.4, 1.5]]\n",
    "\n",
    "# Define the bias as a list\n",
    "bias = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Define the output as a list of zeros\n",
    "output = [0, 0, 0, 0, 0]\n",
    "\n",
    "# Perform matrix multiplication with list comprehensions and built-in Python functionality\n",
    "for i in range(len(input_matrix)):\n",
    "    for j in range(len(weight_matrix[0])):\n",
    "        output[j] += input_matrix[i][j] * weight_matrix[i][j]\n",
    "\n",
    "# Add the bias to the output\n",
    "for j in range(len(output)):\n",
    "    output[j] += bias[j]\n",
    "\n",
    "print(output)  # This will print the output of the dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f78221",
   "metadata": {},
   "source": [
    "### 5.\tWhat is the “hidden size” of a layer?\n",
    "The \"hidden size\" of a layer refers to the number of neurons in that layer. For example, a dense layer with a hidden size of 10 would have 10 neurons.\n",
    "\n",
    "### 6.\tWhat does the t method do in PyTorch?\n",
    "The t method in PyTorch is used to transpose a tensor.\n",
    "\n",
    "### 7.\tWhy is matrix multiplication written in plain Python very slow?\n",
    "Matrix multiplication written in plain Python is slow because it involves nested loops and repeated indexing operations, which can be very inefficient. In particular, computing the dot product of two vectors or matrices requires multiplying each element in one vector or matrix by each corresponding element in the other, and then summing the products. In plain Python, this requires explicitly iterating over all the elements and performing the contrast, optimized libraries like NumPy and PyTorch use highly optimized algorithms and data structures to perform matrix multiplication much more efficiently. For example, NumPy uses C code under the hood to perform matrix multiplication, which can be orders of magnitude faster than plain Python.\n",
    "\n",
    "Additionally, NumPy and PyTorch can take advantage of hardware acceleration like SIMD (Single Instruction Multiple Data) and GPU (Graphics Processing Unit) parallelism to further speed up matrix multiplication. By using these optimized libraries, you can avoid the performance bottleneck of plain Python and take advantage of the performance benefits of optimized code.\n",
    "\n",
    "In summary, matrix multiplication written in plain Python is slow due to the inefficiency of nested loops and repeated indexing operations, while optimized libraries like NumPy and PyTorch use highly optimized algorithms and data structures to perform matrix multiplication much more efficiently.\n",
    "\n",
    "### 8.\tIn matmul, why is ac==br?\n",
    "In matrix multiplication, two matrices can only be multiplied if the number of columns in the first matrix is equal to the number of rows in the second matrix. This is why the dimensions of the matrices being multiplied need to satisfy ac x cb, where 'a' is the number of rows in the first matrix, 'c' is the number of columns in the first matrix (which should be equal to the number of rows in the second matrix), and 'b' is the number of columns in the second matrix.\n",
    "\n",
    "Therefore, if we have matrices A with shape (a, c) and B with shape (c, b), the resulting matrix C from their multiplication will have shape (a, b). This is why ac == br for two matrices to be multiplied using matmul.\n",
    "\n",
    "### 9.\tIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "In Jupyter Notebook, you can measure the time taken for a single cell to execute using the %%time magic command. This command should be added to the beginning of the cell that you want to measure the execution time for. When the cell is executed, the %%time command will print the time taken to run the cell in seconds.\n",
    "\n",
    "Additionally, you can use the %%timeit magic command to measure the average time taken for a cell to execute over multiple runs. This command should also be added to the beginning of the cell that you want to measure the execution time for, and it will automatically run the cell multiple times and print the average execution time.\n",
    "\n",
    "### 10.\tWhat is elementwise arithmetic?\n",
    "Elementwise arithmetic is a mathematical operation that is performed independently on each element of a matrix or tensor. In other words, each element in the first matrix or tensor is combined with the corresponding element in the second matrix or tensor using the specified arithmetic operation. For example, if we have two matrices A and B, then the elementwise sum of the two matrices is obtained by adding the corresponding elements in each matrix, i.e., C[i,j] = A[i,j] + B[i,j]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e5051",
   "metadata": {},
   "source": [
    "### 11.\tWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 1])\n",
    "\n",
    "result = torch.all(torch.gt(a, b))\n",
    "print(result) # This will print True since every element of a is greater than the corresponding element of b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0d553",
   "metadata": {},
   "source": [
    "### 12.\tWhat is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "A rank-0 tensor, also known as a scalar or a 0-dimensional tensor, is a tensor with no dimensions. In PyTorch, a rank-0 tensor is created by passing a single value to the torch.tensor() function. For example, x = torch.tensor(42) creates a rank-0 tensor with the value 42.\n",
    "\n",
    "To convert a rank-0 tensor to a plain Python data type, you can use the .item() method. For example, x.item() would return the value 42 as a Python integer.\n",
    "\n",
    "### 13.\tHow does elementwise arithmetic help us speed up matmul?\n",
    "Elementwise arithmetic helps us speed up matmul by allowing us to use optimized vectorized operations to perform elementwise multiplication, addition, and other arithmetic operations on the tensors before the final matrix multiplication. This can be much faster than performing the same operations using loops, especially for large tensors.\n",
    "\n",
    "### 14.\tWhat are the broadcasting rules?\n",
    "Broadcasting rules refer to how arrays of different shapes are treated during arithmetic operations. In NumPy and PyTorch, broadcasting rules are used to align arrays of different shapes so that they can be used in arithmetic operations.\n",
    "Broadcasting follows a strict set of rules to determine the interaction between the two arrays:\n",
    "\n",
    "If the two arrays differ in their number of dimensions, the shape of the array with fewer dimensions is padded with ones on its leading (left) side.\n",
    "If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.\n",
    "If in any dimension the sizes disagree and neither is equal to 1, an error is raised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc992b93",
   "metadata": {},
   "source": [
    "### 15.\tWhat is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "expand_as is a method in PyTorch that allows a tensor to be expanded to match the shape of another tensor. This is useful in cases where you want to perform operations between tensors with different shapes, but you want to align them first using broadcasting rules. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two tensors\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([10, 20, 30])\n",
    "\n",
    "# Use expand_as to expand y to match the shape of x\n",
    "expanded_y = y.expand_as(x)\n",
    "\n",
    "# Now we can perform arithmetic operations between x and expanded_y\n",
    "z = x + expanded_y\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c0442",
   "metadata": {},
   "source": [
    "Here, we first create two tensors x and y. x is a 2-dimensional tensor with shape (2, 3) and y is a 1-dimensional tensor with shape (3,). We want to add x and y together, but because their shapes are different, we need to use broadcasting rules. We use the expand_as method to expand y to match the shape of x. This creates a new tensor expanded_y with shape (2, 3) that is identical to x. We can then perform the addition operation between x and expanded_y to get the desired result z."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
