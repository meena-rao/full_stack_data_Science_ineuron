{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360e3b05",
   "metadata": {},
   "source": [
    "# Assignment 01 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbb0c3",
   "metadata": {},
   "source": [
    "### 1.\tWhat is the function of a summation junction of a neuron? What is threshold activation function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027de3f",
   "metadata": {},
   "source": [
    "The summation junction, also known as the dendritic tree, is a part of the neuron that receives input signals from other neurons through its dendrites. \n",
    "These input signals are then integrated at the summation junction to generate an overall electrical signal. This signal is then transmitted down the axon of the\n",
    "neuron to its synapses, where it can be passed on to other neurons.\n",
    "\n",
    "The threshold activation function is a non-linear function that is commonly used in artificial neural networks and models of biological neurons. \n",
    "It determines whether the input signal received at the neuron's summation junction is strong enough to trigger an action potential, which is a brief\n",
    "electrical signal that is propagated down the axon of the neuron. If the input signal is below the threshold value, then the neuron will not fire an action potential. \n",
    "If the input signal is above the threshold value, then the neuron will fire an action potential. The threshold activation function helps to ensure that neurons in \n",
    "the brain are only activated in response to meaningful input signals, and not in response to random noise or other irrelevant stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3927b2",
   "metadata": {},
   "source": [
    "### 2.\tWhat is a step function? What is the difference of step function with threshold function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc64b19",
   "metadata": {},
   "source": [
    "The step function being a discrete, non-differentiable function, and the threshold function being a continuous, differentiable function\n",
    "A step function is a mathematical function that has a constant value for a certain range of input values, and then suddenly changes to a different constant value \n",
    "when the input value crosses a specific threshold. The step function can be written mathematically as:\n",
    "\n",
    "f(x) = { 0 for x < c, 1 for x >= c }\n",
    "\n",
    "where c is the threshold value, and the function takes on the value of 0 for all input values less than c, and the value of 1 for all input values greater than or \n",
    "equal to c.\n",
    "\n",
    "The threshold function, on the other hand, is a more general type of activation function used in artificial neural networks that can have a continuous range of\n",
    "output values. It is a function that takes an input signal and produces an output signal based on whether the input signal is greater than or equal to a threshold \n",
    "value. Mathematically, it can be written as:\n",
    "\n",
    "f(x) = { 0 for x < c, x for x >= c }\n",
    "\n",
    "where c is the threshold value, and the function takes on the value of 0 for all input values less than c, and takes on the input value x for all input \n",
    "values greater than or equal to c.\n",
    "\n",
    "In summary, the main difference between a step function and a threshold function is that a step function has a sudden, discrete change in output value \n",
    "at the threshold value, whereas a threshold function has a continuous, linear change in output value above the threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf70d1a",
   "metadata": {},
   "source": [
    "### 3.\tExplain the McCulloch–Pitts model of neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae753f5e",
   "metadata": {},
   "source": [
    "The McCulloch-Pitts model of neuron is a simple mathematical model of a biological neuron proposed by Warren McCulloch and Walter Pitts in 1943. \n",
    "The model is based on the idea that a neuron can be viewed as a simple logical device that processes input signals and produces an output signal based on \n",
    "a set of rules.\n",
    "The McCulloch-Pitts model is a simplified version of the biological neuron, but it provides a useful framework for understanding how neurons process information.\n",
    "It is also the basis for the development of modern artificial neural networks, which are used in a variety of applications, including pattern recognition, image and\n",
    "speech processing, and robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1cd5a9",
   "metadata": {},
   "source": [
    "### 4.\tExplain the ADALINE network model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ca03a",
   "metadata": {},
   "source": [
    "ADALINE (Adaptive Linear Neuron) is a type of artificial neural network that was first introduced by Bernard Widrow and Ted Hoff in 1960. It is a single-layer feedforward network that consists of an input layer, a linear activation function, and an output layer. ADALINE is similar to the Perceptron algorithm, but instead of a threshold activation function, it uses a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea290da2",
   "metadata": {},
   "source": [
    "### 5.\tWhat is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "The simple perceptron is a type of artificial neural network that consists of a single layer of neurons, and it uses a threshold activation function to make \n",
    "binary decisions. The perceptron has a major constraint, which is that it can only classify linearly separable data. In other words, the perceptron can only \n",
    "accurately classify data that can be separated by a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a307eb",
   "metadata": {},
   "source": [
    "### 6.\tWhat is linearly inseparable problem? What is the role of the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9d82e",
   "metadata": {},
   "source": [
    "A linearly inseparable problem is a classification problem where the data cannot be separated by a linear boundary or hyperplane in the input space. In other words, there is no single straight line, plane, or hyperplane that can accurately separate the data points into distinct classes.\n",
    "The hidden layer is a layer of neurons that sit between the input layer and the output layer. The role of the hidden layer is to transform the input data into a higher-dimensional space where it becomes linearly separable. This is achieved by applying a non-linear activation function to the weighted sum of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039059f7",
   "metadata": {},
   "source": [
    "### 7.\tExplain XOR problem in case of a simple perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca80140",
   "metadata": {},
   "source": [
    "The XOR problem is an example of a linearly inseparable problem that cannot be solved by a simple perceptron, but can be solved by a neural network with at least one hidden layer and non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278c8ef",
   "metadata": {},
   "source": [
    "### 8.\tDesign a multi-layer perceptron to implement A XOR B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [0, 1, 1, 0]\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Predict the output\n",
    "print(model.predict([[0, 0], [0, 1], [1, 0], [1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "o/p\n",
    "[[0.03636432]  0\n",
    " [0.9444969 ]  1\n",
    " [0.94442534]  1\n",
    " [0.08000888]]  0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c57c6e",
   "metadata": {},
   "source": [
    "### 9.\tExplain the single-layer feed forward architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26534d66",
   "metadata": {},
   "source": [
    "The single-layer feedforward architecture of artificial neural networks (ANNs) is a type of neural network that consists of a single layer of neurons, also called perceptrons, that are arranged in a feedforward manner. In this architecture, each neuron in the input layer is connected to each neuron in the output layer through weighted connections.\n",
    "\n",
    "The input layer receives the input values, which are fed forward through the network and used to compute the output values of the output layer. Each neuron in the output layer computes its output by taking a linear combination of the inputs from the input layer, multiplied by the weights of the connections between the input layer and the output layer, and passing the result through an activation function.\n",
    "\n",
    "The activation function can be any non-linear function, such as the sigmoid function, the hyperbolic tangent function, or the rectified linear unit (ReLU) function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3722c7",
   "metadata": {},
   "source": [
    "### 10.\tExplain the competitive network architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0008b",
   "metadata": {},
   "source": [
    "The competitive network architecture of artificial neural networks (ANNs) is a type of neural network that is used for unsupervised learning and clustering. This architecture consists of a single layer of neurons, where each neuron competes with all the other neurons in the network for activation.\n",
    "\n",
    "In a competitive network, each neuron receives the same input and computes its output using a linear combination of the inputs, multiplied by its own set of weights. The neuron with the highest output is selected as the winner, and its output is set to one, while the outputs of all other neurons are set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d573a",
   "metadata": {},
   "source": [
    "### 11.\tConsider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eab258",
   "metadata": {},
   "source": [
    "The backpropagation algorithm is a commonly used optimization algorithm for training multi-layer feedforward neural networks. It consists of the following steps:\n",
    "\n",
    "Initialization: Initialize the weights and biases of the network randomly or using some pre-defined values.\n",
    "\n",
    "Forward propagation: Feed the input data through the network, and compute the output of each neuron in the network using the current weights and biases.\n",
    "\n",
    "Calculate error: Calculate the difference between the predicted output of the network and the actual output of the network, also known as the error.\n",
    "\n",
    "Backward propagation: Compute the gradient of the error with respect to the weights and biases of the network, by applying the chain rule of calculus to propagate the error back through the network.\n",
    "\n",
    "Update weights: Use the gradient to update the weights and biases of the network, using an optimization algorithm such as stochastic gradient descent.\n",
    "\n",
    "Repeat: Repeat steps 2-5 for each input in the training data, and for a specified number of iterations or until convergence is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753d7e2",
   "metadata": {},
   "source": [
    "### 12.\tWhat are the advantages and disadvantages of neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb1036",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "Non-linearity: Neural networks are capable of modeling complex, non-linear relationships between input and output variables. They are particularly useful in tasks where traditional linear models, such as regression or linear classification, may not be sufficient.\n",
    "\n",
    "Robustness: Neural networks are relatively robust to noisy data and can generalize well to unseen data, provided they have been trained on a representative sample of the data.\n",
    "\n",
    "Parallel processing: Neural networks can perform computations in parallel, which makes them well-suited to large-scale, high-dimensional problems.\n",
    "\n",
    "Flexibility: Neural networks can be adapted to a wide variety of tasks, including classification, regression, clustering, and anomaly detection, among others.\n",
    "\n",
    "Feature extraction: Neural networks can automatically learn relevant features from the input data, reducing the need for manual feature engineering.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Neural networks can be very complex and difficult to understand, particularly when they have many layers and hidden units. This can make them difficult to interpret and debug.\n",
    "\n",
    "Black box nature: Neural networks are often described as \"black boxes,\" meaning that it can be difficult to understand how they arrive at their predictions. This can make them unsuitable for applications where interpretability is important, such as in medicine or finance.\n",
    "\n",
    "Overfitting: Neural networks are prone to overfitting, meaning that they may learn to model noise in the training data rather than the underlying signal. This can result in poor generalization performance on new data.\n",
    "\n",
    "Computationally intensive: Neural networks can be computationally intensive to train, particularly if they have many layers and hidden units, or if the input data is high-dimensional.\n",
    "\n",
    "Data requirements: Neural networks require a large amount of labeled data to train effectively. This can be a limitation in domains where labeled data is scarce or expensive to obtain.\n",
    "\n",
    "In summary, neural networks have several advantages, including their ability to model complex, non-linear relationships, their robustness to noisy data, and their flexibility. However, they also have several disadvantages, including their complexity, black box nature, tendency to overfit, computational intensity, and data requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bc819",
   "metadata": {},
   "source": [
    "### 13.\tWrite short notes on any two of the following:\n",
    "1.\tBiological neuron\n",
    "2.\tReLU function\n",
    "3.\tSingle-layer feed forward ANN\n",
    "4.\tGradient descent\n",
    "5.\tRecurrent networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52191621",
   "metadata": {},
   "source": [
    "GRADIENT DESCENT\n",
    "It is an iterative optimization algorithm used to find the minimum of a cost function. It is a widely used method for training machine learning models, including neural networks.\n",
    "\n",
    "The idea behind gradient descent is to take small steps in the direction of the steepest decrease in the cost function. At each step, the gradient of the cost function is calculated with respect to the model parameters (weights and biases), and the parameters are updated in the opposite direction of the gradient.\n",
    "\n",
    "Formally, the update rule for the parameters in gradient descent is:\n",
    "\n",
    "θ = θ - α ∇J(θ)\n",
    "\n",
    "where θ is the vector of parameters, J(θ) is the cost function, ∇J(θ) is the gradient of the cost function with respect to θ, and α is the learning rate, which controls the size of the steps taken in the direction of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9a0d4",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) \n",
    "These are a type of neural network that are well-suited to modeling sequential data. Unlike feedforward networks, which process data in a fixed order and do not have any internal memory, RNNs have internal memory that allows them to process sequences of variable length.\n",
    "\n",
    "The key feature of an RNN is its recurrent connection, which allows information to be passed from one time step to the next. At each time step, the RNN takes an input vector and a hidden state vector as input, and produces an output vector and a new hidden state vector as output. The hidden state vector represents the internal memory of the network and encodes information about previous time steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
