{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d32432",
   "metadata": {},
   "source": [
    "### FSDS DL Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3aee9e",
   "metadata": {},
   "source": [
    "### 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448e249",
   "metadata": {},
   "source": [
    "Initializing all weights to the same value using He initialization is not recommended as it will result in all neurons in a layer learning the same features, which reduces the effectiveness of the neural network.\n",
    "\n",
    "He initialization is designed to randomly initialize the weights such that the mean and variance of the activations remain constant throughout the forward and backward passes of the network. It achieves this by sampling from a Gaussian distribution with mean 0 and variance 2/n, where n is the number of input neurons in the layer.\n",
    "\n",
    "By initializing the weights randomly using He initialization, the network is able to learn diverse features that are more effective in solving the problem at hand.\n",
    "\n",
    "Therefore, it is not recommended to initialize all the weights to the same value, even if that value is selected randomly using He initialization. Instead, it is recommended to use He initialization to randomly initialize each weight with a different value sampled from a Gaussian distribution with a mean of 0 and a variance of 2/n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21de2a7",
   "metadata": {},
   "source": [
    "### 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce577b",
   "metadata": {},
   "source": [
    "Yes, it is generally considered okay to initialize the bias terms to 0 in most cases. This is because the role of bias terms in neural networks is to shift the activation function, which can be achieved effectively by initializing the bias terms to 0.\n",
    "\n",
    "In fact, initializing the biases to non-zero values can sometimes lead to convergence issues, as the network may become biased towards certain outputs or features.\n",
    "\n",
    "However, there are some cases where it may be beneficial to initialize the biases to non-zero values, such as in cases where the input data is sparse or the network has a large number of layers. In these cases, initializing the biases to small positive values can help ensure that the neurons in each layer remain active and that the gradients do not vanish during backpropagation.\n",
    "\n",
    "Overall, initializing the biases to 0 is a reasonable default option that is likely to work well in most cases. However, it's always a good idea to experiment with different initialization methods and find the one that works best for your specific problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b96d4",
   "metadata": {},
   "source": [
    "### 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c8784",
   "metadata": {},
   "source": [
    "The SELU (Scaled Exponential Linear Unit) activation function has several advantages over the ReLU (Rectified Linear Unit) activation function, including:\n",
    "\n",
    "Better performance: The SELU activation function has been shown to outperform ReLU on a variety of tasks, particularly on deep neural networks with many layers. This is because SELU can maintain a constant mean and variance of the activations throughout the network, which can help prevent the vanishing or exploding gradient problem.\n",
    "\n",
    "Self-normalization: The SELU activation function is designed to be self-normalizing, which means that the activations in the network tend to converge towards a mean of 0 and a standard deviation of 1. This can lead to faster convergence and better generalization performance, as the network is able to better utilize the available signal in the data.\n",
    "\n",
    "Sparsity preservation: Unlike ReLU, the SELU activation function is able to preserve sparsity in the activations, which can be important for certain types of data and architectures. Sparsity can help reduce the computational cost of the network by reducing the number of active neurons in each layer, which can be particularly beneficial for large networks with many parameters.\n",
    "\n",
    "Overall, the SELU activation function is a powerful tool for deep learning that can provide better performance and faster convergence than ReLU, particularly on deep networks with many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b8f25",
   "metadata": {},
   "source": [
    "### 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e4200",
   "metadata": {},
   "source": [
    "SELU (Scaled Exponential Linear Unit): SELU is a good choice for deep neural networks with many layers, as it is designed to maintain a constant mean and variance of the activations throughout the network, which can help prevent the vanishing or exploding gradient problem. It is also self-normalizing, which can lead to faster convergence and better generalization performance. However, it requires that the input data is normalized and that the weights are initialized in a certain way, so it may not be appropriate for all cases.\n",
    "\n",
    "Leaky ReLU and its variants (e.g., ELU): Leaky ReLU and its variants are a good choice when we want to avoid the \"dying ReLU\" problem, where a large number of neurons may become inactive and produce zero-valued outputs during training. Leaky ReLU and its variants allow for a small, non-zero output for negative inputs, which can help prevent this problem. ELU (Exponential Linear Unit) is a variant of Leaky ReLU that can produce negative outputs as well, which can be beneficial in certain cases.\n",
    "\n",
    "ReLU (Rectified Linear Unit): ReLU is a popular choice for many deep learning tasks, particularly for image recognition and other computer vision tasks. It is computationally efficient, as it simply outputs the input if it is positive and zero otherwise. However, it can suffer from the \"dying ReLU\" problem if the learning rate is too high or the initial weights are not well-tuned.\n",
    "\n",
    "Tanh (Hyperbolic Tangent): Tanh is a good choice for tasks that require outputs in the range of -1 to 1, such as sentiment analysis or speech recognition. It is similar to logistic function, but its output range is shifted to the range of -1 to 1. It can be useful for tasks that require a stronger activation function than ReLU or its variants.\n",
    "\n",
    "Logistic (Sigmoid): Logistic is a good choice for binary classification tasks, where the output is a probability between 0 and 1. It is often used as the activation function for the output layer in binary classification problems.\n",
    "\n",
    "Softmax: Softmax is a good choice for multi-class classification tasks, where the output is a probability distribution over multiple classes. It is often used as the activation function for the output layer in multi-class classification problems.\n",
    "\n",
    "In practice, the choice of activation function depends on the specific task at hand, as well as the properties of the data and the network architecture. It is often a good idea to experiment with different activation functions and architectures to find the one that works best for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd81be",
   "metadata": {},
   "source": [
    "### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad462547",
   "metadata": {},
   "source": [
    "When using Stochastic Gradient Descent (SGD) optimizer with momentum, the momentum hyperparameter controls the contribution of the previous gradient direction to the current update. A higher momentum means that the optimizer relies more on the previous direction and less on the current direction, resulting in smoother updates that can help the optimizer to escape from local minima and converge faster.\n",
    "\n",
    "However, if the momentum hyperparameter is set too close to 1 (e.g., 0.99999), it can cause the optimizer to overshoot the minimum and oscillate around it, or even diverge. This is because the high momentum can cause the optimizer to accumulate too much momentum over time and prevent it from adjusting its direction to follow the gradient of the loss function. As a result, the optimizer can start bouncing back and forth between the opposite directions, unable to converge to the minimum.\n",
    "\n",
    "Therefore, it's important to choose an appropriate momentum hyperparameter that balances between the benefits of smooth updates and the risk of overshooting. A typical range for the momentum hyperparameter is between 0.9 and 0.99, depending on the dataset and model architecture. It's recommended to experiment with different values and monitor the training progress to find the optimal hyperparameters for a particular task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba7c5d",
   "metadata": {},
   "source": [
    "### 8.\tPractice training a deep neural network on the CIFAR10 image dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309da02a",
   "metadata": {},
   "source": [
    "#### a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fa39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Build the deep neural network\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e6fb0",
   "metadata": {},
   "source": [
    "#### b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Define the deep neural network\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model with Nadam optimizer and early stopping\n",
    "optimizer = keras.optimizers.Nadam(lr=0.001)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a073f8",
   "metadata": {},
   "source": [
    "This code uses the Nadam optimizer with a learning rate of 0.001, and the EarlyStopping callback with a patience of 10 epochs to stop the training early if the validation loss doesn't improve for 10 consecutive epochs. The model is compiled with the categorical cross-entropy loss function, accuracy as the evaluation metric, and trained for 100 epochs on the training set, with the validation set used for evaluation after each epoch.\n",
    "\n",
    "To search for the right learning rate, you can use the learning rate finder technique, which involves gradually increasing the learning rate during training and monitoring the loss. Here's an example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d512a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_lr=1e-5, max_lr=1e-1):\n",
    "    num_batches = math.ceil(len(X) / batch_size)\n",
    "    factor = (max_lr / min_lr) ** (1 / (epochs * num_batches))\n",
    "    lr = min_lr\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.SGD(lr=lr, momentum=0.9), metrics=[\"accuracy\"])\n",
    "    losses, lrs = [], []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(num_batches):\n",
    "            idx_start = batch * batch_size\n",
    "            idx_end = (batch + 1) * batch_size\n",
    "            X_batch, y_batch = X[idx_start:idx_end], y[idx_start:idx_end]\n",
    "            history = model.train_on_batch(X_batch, y_batch)\n",
    "            losses.append(history[0])\n",
    "            lrs.append(lr)\n",
    "            lr *= factor\n",
    "            model.optimizer.lr = lr\n",
    "    return losses, lrs\n",
    "\n",
    "# Test the learning rate finder on a small subset of the CIFAR10 dataset\n",
    "X_small, y_small = X_train[:1000], y_train[:1000]\n",
    "model_small = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "losses, lrs = find_learning_rate(model_small, X_small, y_small, epochs=5, batch_size=32, min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91518b9",
   "metadata": {},
   "source": [
    "#### c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6864e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d2c30",
   "metadata": {},
   "source": [
    "    To compare the learning curves with and without batch normalization, we can train both models using Nadam optimization and early stopping and plot their learning curves. Here's an example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eceee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load CIFAR10 data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "y_train_full = y_train_full.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# Split validation set\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Define DNN without batch normalization\n",
    "model_without_bn = keras.models.Sequential()\n",
    "model_without_bn.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model_without_bn.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model_without_bn.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model_without_bn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                         optimizer=keras.optimizers.Nadam(learning_rate=5e-4),\n",
    "                         metrics=[\"accuracy\"])\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "history_without_bn = model_without_bn.fit(X_train, y_train, epochs=100,\n",
    "                                           validation_data=(X_valid, y_valid),\n",
    "                                           callbacks=[early_stopping_cb])\n",
    "\n",
    "# Define DNN with batch normalization\n",
    "model_with_bn = keras.models.Sequential()\n",
    "model_with_bn.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model_with_bn.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model_with_bn.add(keras.layers.BatchNormalization())\n",
    "    model_with_bn.add(keras.layers.Activation(\"elu\"))\n",
    "model_with_bn.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model_with_bn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=keras.optimizers.Nadam(learning_rate=5e-4),\n",
    "                      metrics=[\"accuracy\"])\n",
    "history_with_bn = model_with_bn.fit(X_train, y_train, epochs=100,\n",
    "                                    validation_data=(X_valid, y_valid),\n",
    "                                    callbacks=[early_stopping_cb])\n",
    "\n",
    "# Plot learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_without_bn.history[\"accuracy\"], label=\"Without BN (training)\")\n",
    "plt.plot(history_with_bn.history[\"accuracy\"], label=\"With BN (training)\")\n",
    "plt.plot(history_without_bn.history[\"val_accuracy\"], label=\"Without BN (validation)\")\n",
    "plt.plot(history_with_bn.history[\"val_accuracy\"], label=\"With BN (validation)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e614a",
   "metadata": {},
   "source": [
    "#### d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a93d22",
   "metadata": {},
   "source": [
    "To replace Batch Normalization with SELU activation function and ensure that the network self-normalizes, we need to make the following adjustments:\n",
    "\n",
    "Standardize the input features: We need to preprocess the input data to have a mean of 0 and a standard deviation of 1. This can be done using the StandardScaler class from scikit-learn.\n",
    "\n",
    "Use LeCun normal initialization: We can use the lecun_normal initializer from Keras to initialize the weights.\n",
    "\n",
    "Use the SELU activation function: We can set the activation function of each hidden layer to SELU.\n",
    "\n",
    "Ensure that the DNN contains only a sequence of dense layers: We can use the Dense class from Keras to create the layers of the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load CIFAR10 data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_full = scaler.fit_transform(X_train_full.reshape(-1, 3072)).reshape(-1, 32, 32, 3)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 3072)).reshape(-1, 32, 32, 3)\n",
    "\n",
    "# Define the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.1, callbacks=[early_stopping_cb])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e196f",
   "metadata": {},
   "source": [
    "#### e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15174c",
   "metadata": {},
   "source": [
    "To regularize the model with alpha dropout, we can simply add an AlphaDropout layer after each hidden layer. Here is an example of how to modify the previous model to use alpha dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AlphaDropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(32, 32, 3)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Hidden layers\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(AlphaDropout(rate=0.1))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced836e",
   "metadata": {},
   "source": [
    "To use MC Dropout, we can simply add the Dropout layer before the output layer, and set the training flag to True when making predictions. Here is an example of how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Create a new model with MC Dropout\n",
    "mc_model = Sequential(model.layers[:-1])\n",
    "mc_model.add(Dropout(rate=0.1))\n",
    "mc_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Make predictions with MC Dropout\n",
    "y_pred = np.stack([mc_model.predict(X_test, training=True) for _ in range(100)])\n",
    "y_mean = np.mean(y_pred, axis=0)\n",
    "y_std = np.std(y_pred, axis=0)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, np.argmax(y_mean, axis=1))\n",
    "print(\"MC Dropout Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e34e9",
   "metadata": {},
   "source": [
    "This code creates a new model that is identical to the previous model, but with a Dropout layer added before the output layer. It then makes predictions using the MC Dropout technique by calling the predict method with the training flag set to True. This causes the dropout layer to be applied during inference, simulating the effect of dropout during training. Finally, the code computes the accuracy of the MC Dropout model by averaging the predictions over 100 runs and taking the most probable class for each sample.\n",
    "\n",
    "Note that using MC Dropout can be computationally expensive, since it requires making multiple predictions for each sample. However, it can often lead to better results than regular dropout, especially when the model is trained on small datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
