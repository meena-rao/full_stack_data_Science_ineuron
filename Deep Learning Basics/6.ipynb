{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b18770",
   "metadata": {},
   "source": [
    "###  FSDS DL Assignment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70393fcd",
   "metadata": {},
   "source": [
    "#### \t1.What are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a7ca3",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are better suited than fully connected Deep Neural Networks (DNNs) for image classification tasks due to the following reasons:\n",
    "\n",
    "__Parameter Efficiency:__ CNNs are designed to exploit the spatial correlations present in images by using convolutional layers. These layers share weights across the spatial dimensions, reducing the number of parameters required to represent the model. This parameter efficiency allows CNNs to scale up to larger datasets and deeper architectures.\n",
    "\n",
    "__Translation Invariance:__ CNNs can detect features in images irrespective of their location. This property is achieved using pooling layers that downsample the feature maps while preserving the essential information. This invariance to translation allows CNNs to generalize better to new images, as the same feature can be detected regardless of where it appears in the image.\n",
    "\n",
    "__Hierarchical Representation:__ CNNs have a hierarchical architecture where early layers detect low-level features such as edges and textures, and later layers combine these features to form high-level representations that are more semantically meaningful. This hierarchy allows CNNs to learn more abstract features that are necessary for classification.\n",
    "\n",
    "__Robustness to Variations:__ CNNs are designed to be robust to variations in the input such as rotation, scaling, and deformation. This robustness is achieved by using data augmentation techniques that generate new images by applying random transformations to the training set.\n",
    "\n",
    "Overall, CNNs are a better choice for image classification tasks due to their ability to exploit spatial correlations, translation invariance, hierarchical representation, and robustness to variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e96cb6",
   "metadata": {},
   "source": [
    "#### 2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.  What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963aa94",
   "metadata": {},
   "source": [
    "The total number of parameters in the CNN can be calculated as follows:\n",
    "\n",
    "The first convolutional layer has 3 × 3 × 3 × 100 + 100 = 2,800 parameters (where 3 × 3 is the kernel size, 3 is the number of input channels (RGB), 100 is the number of output channels, and 100 is the bias term).\n",
    "The second convolutional layer has 3 × 3 × 100 × 200 + 200 = 180,200 parameters (where 3 × 3 is the kernel size, 100 is the number of input channels (from the previous layer), 200 is the number of output channels, and 200 is the bias term).\n",
    "The third convolutional layer has 3 × 3 × 200 × 400 + 400 = 1,160,400 parameters (where 3 × 3 is the kernel size, 200 is the number of input channels (from the previous layer), 400 is the number of output channels, and 400 is the bias term).\n",
    "Therefore, the total number of parameters in the CNN is:\n",
    "\n",
    "2,800 + 180,200 + 1,160,400 = 1,343,400 parameters.\n",
    "\n",
    "To calculate the RAM required, we need to consider the size of the weights and biases in the network. Assuming we are using 32-bit floats for both, the RAM required to store the weights and biases for the entire network is:\n",
    "\n",
    "1,343,400 * 32 * 2 / 8 / 1024 / 1024 = 20.36 MB (where we multiply by 2 because we need to store the weights and biases separately).\n",
    "\n",
    "When making a prediction for a single instance, we only need to store the weights and biases for one instance of the network, which is:\n",
    "\n",
    "20.36 MB / 50 = 0.41 MB (where we divide by 50 to get the RAM required for a single instance of the network during training on a mini-batch of 50 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f8747",
   "metadata": {},
   "source": [
    "#### 3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de46dd",
   "metadata": {},
   "source": [
    "If your GPU runs out of memory while training a CNN, there are several things you could try to solve the problem:\n",
    "\n",
    "__Reduce the batch size:__ The batch size is the number of training examples processed in one forward/backward pass. Reducing the batch size can reduce the memory requirements of the network and allow it to fit into the GPU memory.\n",
    "\n",
    "__Reduce the input image size:__ Reducing the size of the input images can also reduce the memory requirements of the network. However, this may result in lower accuracy if the image resolution is reduced too much.\n",
    "\n",
    "__Use smaller or shallower models:__ Smaller or shallower models generally have fewer parameters and require less memory. You can try using a smaller model or reducing the number of layers in the network.\n",
    "\n",
    "__Use mixed precision training: __ Mixed precision training involves using a combination of lower-precision (e.g., half-precision) floating-point numbers and higher-precision (e.g., single-precision) floating-point numbers to reduce the memory requirements of the network.\n",
    "\n",
    "__Use gradient checkpointing:__  Gradient checkpointing is a technique that allows you to trade-off compute time for memory usage by recomputing intermediate activations during the backward pass, rather than storing them in memory.\n",
    "\n",
    "It is also important to note that if the above techniques do not work, you may need to consider upgrading your GPU with more memory, or using a distributed training approach across multiple GPUs or nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39033b82",
   "metadata": {},
   "source": [
    "#### 4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92614daa",
   "metadata": {},
   "source": [
    "Adding a max pooling layer rather than a convolutional layer with the same stride can have several benefits:\n",
    "\n",
    "__Reduction of spatial dimensions:__  Max pooling layers reduce the spatial dimensions of the feature maps by down-sampling the input. This can be useful for reducing the memory requirements of the network and preventing overfitting.\n",
    "\n",
    "__Translation invariance:__ Max pooling layers introduce a degree of translation invariance to the network, which means that the network can recognize the same pattern in different locations of the image. This can be especially useful for tasks such as object recognition, where the location of the object in the image may vary.\n",
    "\n",
    "__Increased robustness to small variations:__ Max pooling layers are able to tolerate small translations or distortions of the input. This means that the network can still recognize the same pattern even if it has been slightly distorted or shifted.\n",
    "\n",
    "__Non-linear activation:__ Max pooling layers introduce a non-linear activation function to the network, which can help to improve the representational power of the network and allow it to learn more complex features.\n",
    "\n",
    "In contrast, a convolutional layer with the same stride would not reduce the spatial dimensions of the feature maps, and may not introduce translation invariance or non-linear activation. Therefore, depending on the specific requirements of the task and the architecture of the network, a max pooling layer may be a more suitable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b22cc",
   "metadata": {},
   "source": [
    "#### 5.\tWhen would you want to add a local response normalization layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581a84d",
   "metadata": {},
   "source": [
    "A local response normalization (LRN) layer is a type of normalization layer that is sometimes added to convolutional neural networks (CNNs) after the activation function. The purpose of an LRN layer is to increase the sparsity of the feature map and improve the discriminative power of the network.\n",
    "\n",
    "Specifically, an LRN layer performs a form of lateral inhibition, where the response of a neuron is normalized by the responses of neighboring neurons. This can help to enhance the contrast between the responses of different neurons and prevent them from saturating or becoming too similar to each other.\n",
    "\n",
    "However, it is important to note that LRN layers have largely been replaced by other normalization techniques in modern CNN architectures, such as batch normalization or group normalization. These techniques have been shown to be more effective and easier to train than LRN layers.\n",
    "\n",
    "In general, it may be useful to consider adding an LRN layer to a CNN if you are working with an older architecture that does not have batch normalization or group normalization, and you are observing poor performance due to overfitting or poor generalization. However, it is recommended to try batch normalization or group normalization first, as they are typically more effective and easier to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922b8cb",
   "metadata": {},
   "source": [
    "#### 6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad89fd1",
   "metadata": {},
   "source": [
    "AlexNet:\n",
    "\n",
    "__Deeper architecture__ : AlexNet had a deeper architecture than LeNet-5, with eight layers (five convolutional and three fully connected).\n",
    "ReLU activation function: AlexNet used the rectified linear unit (ReLU) activation function instead of the sigmoid function used in LeNet-5. ReLU was found to be faster to compute and less prone to the vanishing gradient problem.\n",
    "Dropout regularization: AlexNet used dropout regularization to reduce overfitting during training.\n",
    "Data augmentation: AlexNet used data augmentation techniques such as random cropping and horizontal flipping to increase the effective size of the training set.\n",
    "GoogLeNet:\n",
    "\n",
    "__Inception module:__  GoogLeNet introduced the inception module, which performs multiple convolutions with different filter sizes and pooling operations in parallel, allowing the network to learn features at multiple scales.\n",
    "1x1 convolutions: GoogLeNet used 1x1 convolutions as a way to reduce the dimensionality of the feature maps and introduce non-linearity.\n",
    "Global average pooling: GoogLeNet used global average pooling to convert the final feature maps into a vector for classification, rather than using fully connected layers.\n",
    "ResNet:\n",
    "\n",
    "__Residual connections:__ ResNet introduced residual connections, which allow the network to bypass some layers and learn residual functions. This helps to prevent the vanishing gradient problem and allows for much deeper architectures.\n",
    "Shortcut connections: ResNet used shortcut connections to add the output of one layer to the input of a later layer, allowing the network to learn more easily from the input and gradients.\n",
    "SENet:\n",
    "\n",
    "__Squeeze-and-excitation blocks:__  SENet introduced squeeze-and-excitation (SE) blocks, which learn to weight the importance of different channels in the feature maps. This helps to focus the network's attention on the most informative channels and improve performance.\n",
    "Channel-wise attention: SENet used channel-wise attention to learn how to selectively emphasize or suppress different feature map channels, depending on their importance to the task.\n",
    "Xception:\n",
    "\n",
    "__Separable convolutions__ : Xception used separable convolutions, which split the convolutional operation into a depthwise convolution followed by a pointwise convolution. This allows for more efficient computation and reduces the number of parameters in the network.\n",
    "Fully convolutional architecture: Xception used a fully convolutional architecture, without any fully connected layers. This allows for greater flexibility in input size and makes the network more suitable for tasks such as object detection and segmentation.\n",
    "\n",
    "These are just some of the main innovations in each of the models, and there are many other details and improvements that contribute to their success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8ccd6",
   "metadata": {},
   "source": [
    "#### 7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968bc00",
   "metadata": {},
   "source": [
    "A fully convolutional network (FCN) is a type of neural network that consists entirely of convolutional layers, with no fully connected layers at the end. FCNs are commonly used for tasks such as semantic segmentation, where the output of the network is a dense pixel-wise labeling of an input image.\n",
    "\n",
    "To convert a dense layer into a convolutional layer, you need to ensure that the number of weights and biases in the dense layer is equal to the number of weights and biases in the corresponding convolutional layer. This can be achieved by reshaping the weights and biases of the dense layer into the shape of a convolutional kernel.\n",
    "\n",
    "For example, if you have a dense layer with 512 units and a preceding convolutional layer with 64 filters of size 3x3, you can convert the dense layer into a convolutional layer by reshaping the weights and biases into a 3x3 kernel with 64 output channels. The weights would have the shape (3, 3, 512, 64) and the biases would have the shape (64,).\n",
    "\n",
    "Once you have converted the dense layer into a convolutional layer, you can add it to the rest of the network as you would any other convolutional layer. This allows you to construct fully convolutional networks that can process input images of any size and produce output maps with the same spatial dimensions as the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13d740",
   "metadata": {},
   "source": [
    "#### 8.\tWhat is the main technical difficulty of semantic segmentation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f8a2b",
   "metadata": {},
   "source": [
    "The main technical difficulty of semantic segmentation is the need to produce a dense pixel-wise labeling of an input image, where each pixel is assigned to one of several pre-defined classes. This requires the network to capture both local and global context, as well as spatial dependencies between adjacent pixels.\n",
    "\n",
    "In contrast to other computer vision tasks such as object detection or image classification, where the output is a single label or a set of bounding boxes, semantic segmentation requires the network to output a high-resolution image with as many pixels as the input image.\n",
    "\n",
    "Achieving accurate segmentation is also challenging due to the inherent ambiguity in image data, where multiple objects or regions can have similar appearances and shapes, and where the same object can appear differently under different conditions such as lighting or occlusion.\n",
    "\n",
    "To address these challenges, semantic segmentation models often use complex architectures that combine multiple convolutional layers with skip connections and upsampling layers to capture fine-grained details and long-range dependencies, as well as techniques such as data augmentation and regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986be81",
   "metadata": {},
   "source": [
    "#### 9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afb86a",
   "metadata": {},
   "source": [
    "Import necessary libraries and load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3af683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e31f1",
   "metadata": {},
   "source": [
    "Preprocess the data by normalizing pixel values between 0 and 1 and reshaping the data to include a channel dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ef5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da629fb",
   "metadata": {},
   "source": [
    "Define the CNN architecture, consisting of 3 convolutional layers followed by 2 dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82121d7b",
   "metadata": {},
   "source": [
    "Compile the model, specifying the loss function, optimizer, and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774200fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec594a",
   "metadata": {},
   "source": [
    "Train the model on the training data for a specified number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c19e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce9dfd",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data and print the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d33b2b",
   "metadata": {},
   "source": [
    "By following these steps, you should be able to train a CNN from scratch for the MNIST dataset and achieve an accuracy of around 99%. Feel free to experiment with different architectures and hyperparameters to try to achieve even higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
