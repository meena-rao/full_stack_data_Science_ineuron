{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26253261",
   "metadata": {},
   "source": [
    "## 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ff0f4",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Explain the Activation Functions in your own language\n",
    "a)\tsigmoid\n",
    "b)\ttanh\n",
    "c)\tReLU\n",
    "d)\tELU\n",
    "e)\tLeakyReLU\n",
    "f)\tswish\n",
    "\n",
    "1. Activation Functions\n",
    "a) Sigmoid: The sigmoid function is a commonly used activation function that maps any input value to a value between 0 and 1. It is useful in binary classification problems where the output can be interpreted as a probability of belonging to a particular class.\n",
    "\n",
    "b) Tanh: The hyperbolic tangent (tanh) function is similar to the sigmoid function, but maps input values to a range between -1 and 1. It is useful in cases where the input can have negative values.\n",
    "\n",
    "c) ReLU: The Rectified Linear Unit (ReLU) function is a widely used activation function that outputs the input directly if it is positive, and 0 if it is negative. It is computationally efficient and helps in reducing the vanishing gradient problem.\n",
    "\n",
    "d) ELU: The Exponential Linear Unit (ELU) function is similar to the ReLU function, but it has a negative slope for negative input values. This helps in avoiding the dying ReLU problem.\n",
    "\n",
    "e) LeakyReLU: The Leaky Rectified Linear Unit (LeakyReLU) function is similar to the ReLU function, but it has a small non-zero slope for negative input values. This helps in addressing the dying ReLU problem.\n",
    "\n",
    "f) Swish: The Swish function is a relatively new activation function that is similar to the sigmoid function, but it uses the input multiplied by a sigmoid function as the output. It has been shown to improve the performance of deep neural networks.\n",
    "\n",
    "### 2. What happens when you increase or decrease the optimizer learning rate?\n",
    "The learning rate of an optimizer determines the step size taken in the direction of the gradient during training. Increasing the learning rate can speed up the training process, but may cause the optimization to overshoot the minimum and diverge. Decreasing the learning rate can lead to slower training but can help the optimization converge to a better minimum.\n",
    "### 3. What happens when you increase the number of internal hidden neurons?\n",
    "Increasing the number of hidden neurons can increase the capacity of the model, allowing it to learn more complex patterns in the data. However, adding too many hidden neurons can cause the model to overfit the training data and perform poorly on new data.\n",
    "### 4. What happens when you increase the size of batch computation?\n",
    "Increasing the batch size can improve the computational efficiency of training as more data can be processed simultaneously. However, using a large batch size can result in a suboptimal solution as the optimization may converge to a local minimum instead of the global minimum.\n",
    "### 5. Why we adopt regularization to avoid overfitting?\n",
    "Regularization is used to avoid overfitting in deep learning by adding a penalty term to the loss function that discourages large weights. This helps in preventing the model from memorizing the training data and generalizing better to new data.\n",
    "### 6. What are loss and cost functions in deep learning?\n",
    "The loss function measures the difference between the predicted output of the model and the actual output. The cost function is the average of the loss function over all the training examples. The aim of training is to minimize the cost function.\n",
    "### 7. What do ou mean by underfitting in neural networks?\n",
    "Underfitting in neural networks occurs when the model is too simple and cannot capture the underlying patterns in the data. This results in poor performance on both the training and test data.\n",
    "### 8. Why we use Dropout in Neural Networks?\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting. It works by randomly dropping out some of the neurons during training, forcing the network to learn more robust features. This helps in improving the generalization performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
