{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b916a8df",
   "metadata": {},
   "source": [
    "## DL 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41b2fe",
   "metadata": {},
   "source": [
    "#### 1.\tWhy is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9af58",
   "metadata": {},
   "source": [
    "Logistic Regression classifiers have several advantages over classical Perceptrons:\n",
    "\n",
    "Logistic Regression can output probabilities, which can be useful in many applications such as binary classification, multi-class classification, and anomaly detection.\n",
    "\n",
    "Logistic Regression can handle non-linearly separable data by using non-linear transformations of the input features.\n",
    "\n",
    "Logistic Regression can be trained using more sophisticated optimization algorithms, such as stochastic gradient descent with momentum or Adam, which can converge faster and more reliably than the Perceptron training algorithm.\n",
    "\n",
    "To make a Perceptron equivalent to a Logistic Regression classifier, we can modify the activation function to be the logistic function (also known as the sigmoid function) instead of the step function. The logistic function has a similar shape to the step function but is continuous and differentiable, which makes it more suitable for optimization algorithms that require gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9387c5",
   "metadata": {},
   "source": [
    "#### 2.\tWhy was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The logistic activation function, also known as the sigmoid function, was a key ingredient in training the first Multilayer Perceptrons (MLPs) for several reasons:\n",
    "\n",
    "The logistic function is a smooth, differentiable function that can be used to model non-linear relationships between the input features and the output targets. This is important because many real-world problems are non-linear and cannot be accurately modeled using linear functions.\n",
    "\n",
    "The logistic function has a simple and interpretable derivative, which makes it easy to compute gradients for training the MLP using backpropagation. Backpropagation is an algorithm for computing the gradients of the loss function with respect to the weights of the MLP, which is used to update the weights during training.\n",
    "\n",
    "The logistic function has a convenient range of output values between 0 and 1, which can be interpreted as probabilities. This makes it well-suited for binary classification tasks, where the goal is to predict a binary output (e.g., yes or no) based on a set of input features.\n",
    "\n",
    "The logistic function was one of the few activation functions that had been studied and understood at the time of the development of MLPs. This made it a natural choice for early researchers in the field.\n",
    "\n",
    "Overall, the logistic activation function was a key ingredient in training the first MLPs because it provided a simple, interpretable, and differentiable non-linear activation function that could be used to model a wide range of real-world problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7e768",
   "metadata": {},
   "source": [
    "#### 3.\tName three popular activation functions. Can you draw them?\n",
    "Sigmoid, Relu, softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4cb6c",
   "metadata": {},
   "source": [
    "#### 4.\tSuppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c110364",
   "metadata": {},
   "source": [
    "##### What is the shape of the input matrix X?\n",
    "(m,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969cf3e",
   "metadata": {},
   "source": [
    "##### What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
    "The shape of the weight vector Wh for the hidden layer would be (10, 50), as there are 10 neurons in the input layer and 50 neurons in the hidden layer, and each input neuron is connected to each hidden neuron. The shape of the bias vector bh for the hidden layer would be (50,), since there is one bias term per hidden neuron.\n",
    "\n",
    "Therefore, the weight matrix for the hidden layer would have a shape of (10, 50), and the bias vector for the hidden layer would have a shape of (50,)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0f8d1",
   "metadata": {},
   "source": [
    "##### What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "(50,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7ac4d",
   "metadata": {},
   "source": [
    "##### What is the shape of the network’s output matrix Y?\n",
    "(m,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e778c7e",
   "metadata": {},
   "source": [
    "##### Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo.\n",
    "The equation that computes the network's output matrix Y as a function of X, Wh, bh, Wo, and bo can be written using matrix multiplication and activation functions as follows:\n",
    "\n",
    "Z1 = X.dot(Wh) + bh\n",
    "H = np.maximum(0, Z1) # ReLU activation function\n",
    "Z2 = H.dot(Wo) + bo\n",
    "Y = softmax(Z2) # Softmax activation function\n",
    "\n",
    "Here, X is the input matrix with shape (m, 10), Wh is the weight matrix for the hidden layer with shape (10, 50), bh is the bias vector for the hidden layer with shape (50,), Wo is the weight matrix for the output layer with shape (50, 3), bo is the bias vector for the output layer with shape (3,), and softmax is the activation function used for the output layer.\n",
    "\n",
    "The ReLU activation function is used for the hidden layer, which is represented as a numpy array H with shape (m, 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30f433",
   "metadata": {},
   "source": [
    "#### 5.\tHow many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?\n",
    "If you want to classify email into spam or ham, you need only 1 neuron in the output layer. You should use the sigmoid activation function in the output layer to produce a probability of the email being spam.\n",
    "\n",
    "If you want to tackle the MNIST classification task, you need 10 neurons in the output layer, one for each digit from 0 to 9. You should use the softmax activation function in the output layer to produce a probability distribution over the 10 possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d846d",
   "metadata": {},
   "source": [
    "### 6.\tWhat is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "Backpropagation is an algorithm for efficiently computing the gradients of the loss function with respect to the weights of a neural network. It works by performing a forward pass through the network to compute the network's predictions, and then performing a backward pass through the network to compute the gradients of the loss function with respect to the network's parameters. These gradients are then used to update the parameters using gradient descent or another optimization algorithm.\n",
    "\n",
    "During the backward pass, the backpropagation algorithm calculates the gradient of the loss function with respect to the output of each layer, and then uses the chain rule of calculus to calculate the gradient of the loss function with respect to the weights and biases of each layer. This calculation is done iteratively, starting from the output layer and working backwards through the network.\n",
    "\n",
    "Reverse-mode autodiff, on the other hand, is a more general algorithm for efficiently computing gradients of a function with respect to its inputs. It can be used to compute the gradients of any function, not just neural networks. Unlike backpropagation, which is specific to neural networks, reverse-mode autodiff works by building a computation graph that represents the function being differentiated and then performing a forward and backward pass through this graph to compute the gradients.\n",
    "\n",
    "In practice, however, the backpropagation algorithm is often implemented using reverse-mode autodiff because it is more efficient than computing gradients using the chain rule directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b47874",
   "metadata": {},
   "source": [
    "### 7.\tCan you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "Here are some hyperparameters that can be tweaked in an MLP:\n",
    "\n",
    "Number of hidden layers\n",
    "\n",
    "Number of neurons per hidden layer\n",
    "\n",
    "Activation function for each layer\n",
    "\n",
    "Learning rate\n",
    "\n",
    "Mini-batch size\n",
    "\n",
    "Number of epochs\n",
    "\n",
    "Regularization parameters (e.g. L1 or L2 regularization)\n",
    "\n",
    "Dropout rate\n",
    "\n",
    "Weight initialization method\n",
    "\n",
    "Optimization algorithm (e.g. stochastic gradient descent, Adam, RMSprop, etc.)\n",
    "\n",
    "If the MLP overfits the training data, here are some possible ways to address the issue by tweaking the hyperparameters:\n",
    "\n",
    "Reduce the number of hidden layers or neurons per layer to reduce the model's complexity.\n",
    "\n",
    "Increase the regularization parameter to penalize large weights and prevent overfitting.\n",
    "\n",
    "Increase the dropout rate to randomly drop out some neurons during training and prevent over-reliance on specific neurons.\n",
    "\n",
    "Decrease the learning rate to slow down the weight updates and prevent overshooting of the minimum.\n",
    "\n",
    "Increase the batch size to increase the generalization of the gradients.\n",
    "\n",
    "Stop training early (early stopping) to prevent the model from overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fb722",
   "metadata": {},
   "source": [
    "### 8.\tTrain a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on).\n",
    "* Load the MNIST dataset.\n",
    "* Preprocess the data (e.g., scale it, reshape it, and so on).\n",
    "* Split the dataset into training, validation, and test sets.\n",
    "* Define the architecture of the MLP, including the number of hidden layers, the number of neurons in each layer, the activation functions, the loss function, and the optimizer.\n",
    "* Train the MLP on the training set, using the validation set to tune the hyperparameters (e.g., the learning rate, the number of epochs, the batch size, and so on).\n",
    "* Evaluate the performance of the MLP on the test set.\n",
    "* Save the trained model and checkpoints.\n",
    "* Restore the last checkpoint in case of an interruption.\n",
    "* Add summaries to monitor the training progress and plot learning curves using TensorBoard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
