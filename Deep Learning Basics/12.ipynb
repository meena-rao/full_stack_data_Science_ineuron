{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baab9da9",
   "metadata": {},
   "source": [
    "## DL 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b87b7",
   "metadata": {},
   "source": [
    "### 1.\tHow does unsqueeze help us to solve certain broadcasting problems?\n",
    "The unsqueeze operation can help us to add a new dimension to a tensor, which can solve certain broadcasting problems where two tensors have different numbers of dimensions.\n",
    "\n",
    "### 2.\tHow can we use indexing to do the same operation as unsqueeze?\n",
    "We can use indexing with None or np.newaxis to achieve the same operation as unsqueeze. For example, x[:, None] will add a new dimension to x along the second axis.\n",
    "\n",
    "### 3.\tHow do we show the actual contents of the memory used for a tensor?\n",
    "We can use the numpy.ndarray's __array__ method to get a reference to the underlying memory buffer, and then use the memoryview function to display the contents of the memory buffer. For example, memoryview(x.__array__()) will show the memory contents of the tensor x.\n",
    "\n",
    "### 4.\tWhen adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "The elements of the vector will be added to each column of the matrix.\n",
    "\n",
    "### 5.\tDo broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "No, broadcasting and expand_as do not result in increased memory use. They only change the way the tensor is viewed or accessed without actually creating a new copy of the data.\n",
    "### 6.\tImplement matmul using Einstein summation.\n",
    "The Einstein summation notation for matrix multiplication can be written as z_ij = np.einsum('ik,kj->ij', x, y).\n",
    "### 7.\tWhat does a repeated index letter represent on the lefthand side of einsum?\n",
    "A repeated index letter on the lefthand side of einsum indicates that the corresponding dimension of the output should be summed over. For example, np.einsum('ii', x) will sum over the diagonal elements of x.\n",
    "\n",
    "### 8.\tWhat are the three rules of Einstein summation notation? Why?\n",
    "The three rules of Einstein summation notation are: (1) repeated indices are implicitly summed over, (2) an index that appears only on the right-hand side of the expression is summed over, and (3) an index that appears exactly twice (once as a subscript and once as a superscript) is not summed over and represents a direct product.\n",
    "\n",
    "### 9.\tWhat are the forward pass and backward pass of a neural network?\n",
    "The forward pass of a neural network refers to the process of computing the output of the network given an input, while the backward pass refers to the process of computing the gradients of the loss function with respect to the weights of the network, which is used to update the weights during training.\n",
    "\n",
    "### 10.\tWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "Storing activations calculated for intermediate layers in the forward pass is necessary for computing the gradients of the loss function with respect to the weights during the backward pass. Without this information, we would not be able to update the weights correctly.\n",
    "\n",
    "### 11.\tWhat is the downside of having activations with a standard deviation too far away from 1?\n",
    "The downside of having activations with a standard deviation too far away from 1 is that it can lead to the vanishing or exploding gradients problem, which can make it difficult to train the network.\n",
    "\n",
    "### 12.\tHow can weight initialization help avoid this problem?\n",
    "Weight initialization can help avoid this problem by setting the initial weights to small random values that are close to zero, which can prevent the activations from being too large or too small and reduce the risk of vanishing or exploding gradients. Some popular weight initialization methods include Xavier initialization and He initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
