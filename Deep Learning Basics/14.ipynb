{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da627044",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bed9bf5",
   "metadata": {},
   "source": [
    "1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "2.\tIs it okay to initialize the bias terms to 0?\n",
    "3.\tName three advantages of the ELU activation function over ReLU.\n",
    "4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "6.\tName three ways you can produce a sparse model.\n",
    "7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602e9cf",
   "metadata": {},
   "source": [
    "1. Initializing all weights to the same value using He initialization is not recommended, as it can lead to symmetry breaking and poor model performance. He initialization selects random values from a Gaussian distribution with mean 0 and variance (2/n), where n is the number of inputs to the layer. By randomly initializing each weight with a different value drawn from this distribution, we can avoid symmetry breaking and promote better performance.\n",
    "\n",
    "2. Yes, it is generally safe to initialize bias terms to 0, as they have less impact on the overall performance of the model compared to weight initialization. However, some researchers have found that initializing biases to non-zero values can help improve model performance in certain cases.\n",
    "\n",
    "3. Three advantages of the ELU activation function over ReLU are:\n",
    "a. ELU produces negative values, which allows the network to have a mean activation closer to zero. This can help improve the gradient flow and reduce the vanishing gradient problem.\n",
    "b. ELU is smooth and continuously differentiable, which can help reduce the likelihood of \"dead neurons\" that do not contribute to the model's output.\n",
    "c. ELU can help prevent overfitting by forcing the activations to be sparse.\n",
    "\n",
    "4. The choice of activation function depends on the specific task and the type of model being used. In general:\n",
    "\n",
    "ELU and leaky ReLU (and its variants) can be good choices for deep neural networks, as they can help address the vanishing gradient problem.\n",
    "ReLU is a good default choice for most situations, as it is fast to compute and works well in practice.\n",
    "tanh and logistic are useful for binary classification tasks, as they can help produce probabilities that range from 0 to 1.\n",
    "softmax is typically used in the output layer of a neural network for multiclass classification tasks, as it produces a probability distribution over the possible classes.\n",
    "\n",
    "5. If the momentum hyperparameter is set too close to 1, the optimizer will place more weight on the past gradients and less on the current gradients. This can cause the optimizer to overshoot the minimum and oscillate around it, leading to slow convergence and poor performance.\n",
    "\n",
    "6. Three ways to produce a sparse model are:\n",
    "a. L1 regularization: This encourages weights to be small and promotes sparsity in the model by setting some weights to zero.\n",
    "b. DropConnect: This is similar to dropout, but instead of dropping out neurons, it drops out weights. This can help create a sparse network.\n",
    "c. Pruning: This involves iteratively removing the smallest weights in the network until a desired level of sparsity is achieved.\n",
    "\n",
    "7. Dropout can slow down training, as it requires more computations to be performed per training iteration. However, it can also help prevent overfitting and improve generalization, which can ultimately lead to faster convergence and better performance. Dropout does not slow down inference, as the dropped-out neurons are only used during training and are reactivated during inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
