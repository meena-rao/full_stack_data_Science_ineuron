{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbd5610",
   "metadata": {},
   "source": [
    "### FSDS DL Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd198b93",
   "metadata": {},
   "source": [
    "#### 1.\tCan you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c83c0c",
   "metadata": {},
   "source": [
    "##### Sequence-to-sequence RNN:\n",
    "Machine translation: where the input is a sequence of words in one language and the output is a sequence of words in another language.\n",
    "Speech recognition: where the input is an audio signal and the output is a sequence of phonemes or words.\n",
    "Text summarization: where the input is a long text and the output is a shorter summary.\n",
    "Chatbot: where the input is a user's message and the output is a response.\n",
    "\n",
    "###### Sequence-to-vector RNN:\n",
    "\n",
    "Sentiment analysis: where the input is a sequence of words and the output is a vector representing the sentiment of the text.\n",
    "Named entity recognition: where the input is a sequence of words and the output is a vector indicating whether each word is a named entity or not.\n",
    "Image captioning: where the input is an image and the output is a vector representing a caption for the image.\n",
    "\n",
    "###### Vector-to-sequence RNN:\n",
    "Music generation: where the input is a vector representing the style or genre of music and the output is a sequence of notes.\n",
    "Text generation: where the input is a vector representing the topic or style of the text and the output is a sequence of words.\n",
    "Video captioning: where the input is a vector representing the content of a video and the output is a sequence of captions for the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3968e87",
   "metadata": {},
   "source": [
    "#### 2.\tHow many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206deb94",
   "metadata": {},
   "source": [
    "The inputs of an RNN layer typically have three dimensions:\n",
    "\n",
    "The batch size: the number of examples in a batch.\n",
    "The time steps: the number of time steps in the sequence.\n",
    "The input features: the number of features at each time step.\n",
    "For example, if we have a batch of 32 sequences, each sequence has 10 time steps, and each time step has a feature vector of size 50, then the input shape would be (32, 10, 50).\n",
    "\n",
    "The first dimension (batch size) allows us to process multiple sequences at once, which is useful for parallelism and faster computation. The second dimension (time steps) allows the RNN to remember information from previous time steps. The third dimension (input features) represents the number of features in each time step.\n",
    "\n",
    "The outputs of an RNN layer also have three dimensions:\n",
    "\n",
    "The batch size: the number of examples in a batch.\n",
    "The time steps: the number of time steps in the sequence.\n",
    "The output features: the number of features in the output at each time step.\n",
    "For example, if we have a batch of 32 sequences, each sequence has 10 time steps, and the RNN outputs a vector of size 100 at each time step, then the output shape would be (32, 10, 100).\n",
    "\n",
    "The third dimension (output features) represents the number of features in the output at each time step. It can be the same or different from the input features. The output at each time step can be used for further processing or passed to another layer in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0426f",
   "metadata": {},
   "source": [
    "#### 3.\tIf you want to build a deep sequence-to-sequence RNN, which RNN layers should have return_sequences=True? What about a sequence-to-vector RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a73b97",
   "metadata": {},
   "source": [
    "If you want to build a deep sequence-to-sequence RNN, then all the RNN layers in the encoder should have return_sequences=True, while all the RNN layers in the decoder should have return_sequences=True except for the last one.\n",
    "\n",
    "In the encoder, each RNN layer takes the output of the previous layer as its input, and since we want to pass the entire sequence to the next layer, we set return_sequences=True. The last layer in the encoder may or may not have return_sequences=True depending on the specific architecture and task requirements.\n",
    "\n",
    "In the decoder, each RNN layer takes the output of the previous layer as its input, but we only want the final output of the decoder, so we set return_sequences=False for the last layer.\n",
    "\n",
    "Here's an example code snippet for a deep sequence-to-sequence RNN with LSTM layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01691da0",
   "metadata": {},
   "source": [
    "#### 4.\tSuppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f10c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "\n",
    "# Define the input sequence\n",
    "encoder_inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "# Define the encoder layers\n",
    "encoder1 = LSTM(units, return_sequences=True)(encoder_inputs)\n",
    "encoder2 = LSTM(units, return_sequences=True)(encoder1)\n",
    "\n",
    "# Define the decoder layers\n",
    "decoder1 = LSTM(units, return_sequences=True)(encoder2)\n",
    "decoder2 = LSTM(units, return_sequences=False)(decoder1)\n",
    "decoder_outputs = RepeatVector(timesteps)(decoder2)\n",
    "\n",
    "# Define the model\n",
    "model = Model(encoder_inputs, decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376a7a6",
   "metadata": {},
   "source": [
    "If you want to build a sequence-to-vector RNN, then the last RNN layer should have return_sequences=False to output a single vector representing the entire sequence. \n",
    "Here's an example code snippet for a sequence-to-vector RNN with LSTM layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa909d53",
   "metadata": {},
   "source": [
    "#### 5.\tWhat are the main difficulties when training RNNs? How can you handle them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "# Define the input sequence\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "# Define the LSTM layers\n",
    "lstm1 = LSTM(units, return_sequences=True)(inputs)\n",
    "lstm2 = LSTM(units, return_sequences=False)(lstm1)\n",
    "\n",
    "# Define the output layer\n",
    "outputs = Dense(output_dim, activation='softmax')(lstm2)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac782a",
   "metadata": {},
   "source": [
    "Note that the output layer can be any type of layer depending on the task, not necessarily a Dense layer with a softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2be2f",
   "metadata": {},
   "source": [
    "#### 6.\tCan you sketch the LSTM cellâ€™s architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ad9f3",
   "metadata": {},
   "source": [
    "                                   +-----------------+                               \n",
    "                                   |      Input      |                               \n",
    "                                   |  (timesteps, d) |                               \n",
    "                                   +--------+--------+                               \n",
    "                                            |                                        \n",
    "                                            |                                        \n",
    "                        +-------------------+-------------------+                    \n",
    "                        |                   |                   |                    \n",
    "             +----------v----------+ +--------v---------+ +---------v---------+      \n",
    "             |       Forget Gate    | |    Input Gate     | |     Output Gate    |      \n",
    "             | (dense layer + sigmoid) | | (dense layer +    | | (dense layer +     |      \n",
    "             |        (d,1)         | |    sigmoid)       | |   sigmoid)       |      \n",
    "             +----------+----------+ +--------+---------+ +---------+---------+      \n",
    "                        |                   |                   |                    \n",
    "                        v                   v                   v                    \n",
    "                +-------+-------+   +-------+-------+   +-------+-------+            \n",
    "                |  Previous Cell  |   |  Candidate for   |   |  Current Output  |            \n",
    "                |      State      |   |   New Memory    |   |   (d-dimensional)|            \n",
    "                |      (d,1)      |   |      (d,1)      |   +-----------------+            \n",
    "                +-----------------+   +-----------------+                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f5b5d",
   "metadata": {},
   "source": [
    "The LSTM cell takes as input the previous cell state and the current input at each time step. It has three gates: the forget gate, the input gate, and the output gate.\n",
    "\n",
    "The forget gate is responsible for deciding which information to discard from the previous cell state. It takes the previous cell state and the current input as inputs and outputs a number between 0 and 1 for each element in the cell state. A value of 0 means \"completely forget this element\" and a value of 1 means \"keep this element as it is\".\n",
    "The input gate is responsible for deciding which new information to add to the cell state. It takes the previous cell state and the current input as inputs and outputs a number between 0 and 1 for each element in the cell state. A value of 0 means \"don't add any new information\" and a value of 1 means \"add this new information completely\".\n",
    "The output gate is responsible for deciding which information to output from the current cell state. It takes the current input and the current cell state as inputs and outputs a number between 0 and 1 for each element in the cell state. A value of 0 means \"don't output this element\" and a value of 1 means \"output this element as it is\".\n",
    "The candidate for new memory is also calculated, which is the candidate for the new values that will be added to the current cell state. The candidate is calculated based on the current input and the previous hidden state.\n",
    "\n",
    "The previous cell state, current input, and the outputs of the forget gate, input gate, and candidate for new memory are used to calculate the new cell state. The new cell state is a weighted sum of the previous cell state and the candidate for new memory, where the weights are given by the output of the forget gate and the input gate, respectively.\n",
    "\n",
    "Finally, the output gate is applied to the new cell state to produce the output of the LSTM cell, which is the current hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002ea3a",
   "metadata": {},
   "source": [
    "#### 7.\tWhy would you want to use 1D convolutional layers in an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7221d7c",
   "metadata": {},
   "source": [
    "1D convolutional layers can be used in an RNN for sequence processing tasks, particularly when the input sequence has some spatial or temporal structure. The use of 1D convolutional layers can help capture local patterns in the sequence that may be relevant for the task at hand.\n",
    "\n",
    "In particular, 1D convolutional layers can be used to extract features from the input sequence before feeding it into the RNN. This can help reduce the dimensionality of the input sequence and highlight relevant patterns that may be useful for the RNN to learn. The 1D convolutional layers can also be used to learn filters that can detect specific features or motifs in the sequence, such as specific DNA sequences in genomics or speech phonemes in speech recognition.\n",
    "\n",
    "Furthermore, 1D convolutional layers can be used to add more depth to the RNN model. By stacking 1D convolutional layers on top of each other before feeding the output into the RNN layers, the model can learn more complex hierarchical features in the input sequence. This can be particularly useful for tasks such as speech recognition or natural language processing, where the input sequences can be quite complex and hierarchical in nature.\n",
    "\n",
    "Overall, the use of 1D convolutional layers in an RNN can improve the model's ability to capture relevant features in the input sequence, reduce the dimensionality of the sequence, and add more depth to the model for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa402def",
   "metadata": {},
   "source": [
    "#### 8.\tWhich neural network architecture could you use to classify videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63383e3a",
   "metadata": {},
   "source": [
    "To classify videos, a popular neural network architecture is the 3D Convolutional Neural Network (CNN). This architecture extends the standard 2D CNN by including a third dimension for processing video frames over time.\n",
    "\n",
    "In a 3D CNN, the input is a sequence of video frames, where each frame is a 3D tensor (width x height x color channels). The 3D CNN applies a set of learnable filters to the entire sequence of frames, which captures spatio-temporal features across the video. The output of the 3D CNN is then fed into a fully connected layer for classification.\n",
    "\n",
    "By processing the video frames over time, the 3D CNN can learn to detect temporal patterns and dependencies in the video data, which can improve the model's ability to classify the video. This makes it particularly useful for applications such as action recognition, where the motion and movement of objects in the video are important cues for classification.\n",
    "\n",
    "Other architectures that can also be used for video classification include two-stream CNNs, which process spatial and temporal information separately, and recurrent neural networks (RNNs), which can model the temporal dependencies in the video data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafcf57",
   "metadata": {},
   "source": [
    "9.\tTrain a classification model for the SketchRNN dataset, available in TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the SketchRNN dataset\n",
    "dataset, info = tfds.load('sketch_rnn/quickdraw', with_info=True)\n",
    "\n",
    "# Define the input shape and number of classes\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_data(sample):\n",
    "    # Normalize the image\n",
    "    image = tf.cast(sample['image'], tf.float32) / 255.0\n",
    "    # Resize the image to the input shape\n",
    "    image = tf.image.resize(image, input_shape[:2])\n",
    "    # Return the preprocessed image and label\n",
    "    return image, tf.one_hot(sample['label'], num_classes)\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "train_dataset = dataset['train'].map(preprocess_data).batch(32).shuffle(10000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12e6ba",
   "metadata": {},
   "source": [
    "In this example, we define a simple CNN architecture consisting of two convolutional layers with ReLU activation, followed by a max pooling layer, a flatten layer, and two dense layers with ReLU and softmax activations, respectively. We compile the model with the categorical cross-entropy loss and the Adam optimizer, and use accuracy as the evaluation metric.\n",
    "\n",
    "We preprocess the dataset by normalizing the images and resizing them to the input shape, and apply the preprocessing function to the training dataset. Finally, we train the model for 10 epochs using the preprocessed training dataset.\n",
    "\n",
    "Note that this is just a simple example, and there are many other ways to preprocess the data and define the model architecture, depending on the specific requirements of your classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
