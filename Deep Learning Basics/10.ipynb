{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985f58e7",
   "metadata": {},
   "source": [
    "## DL 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa833fd",
   "metadata": {},
   "source": [
    "### 1.\tWhat does a SavedModel contain? How do you inspect its content?\n",
    "\n",
    "A SavedModel is a format for saving TensorFlow models that allows for easy reloading and sharing of the model. It contains several elements, including the model's architecture, variables (i.e., the model's learned parameters), and assets such as lookup tables or vocabulary files.\n",
    "\n",
    "### 2.\tWhen should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "TF Serving is a great choice when you want to deploy a machine learning model in a production environment where it needs to be accessed by multiple clients. Some of its main features include support for serving TensorFlow models in multiple languages and platforms, the ability to handle multiple models at the same time, and the ability to scale horizontally to handle high traffic loads. Some tools that can be used to deploy TF Serving include Kubernetes, Docker, and Ansible.\n",
    "\n",
    "\n",
    "### 3.\tHow do you deploy a model across multiple TF Serving instances?\n",
    "To deploy a model across multiple TF Serving instances, you can use the --cluster flag when starting each instance, which specifies the addresses of the other instances in the cluster. You can also use the --model_config_file flag to specify a configuration file that lists the models to be served and the instances that should serve them. TF Serving uses a consistent hashing algorithm to distribute requests among the instances in the cluster, which ensures that requests for the same input are always sent to the same instance.\n",
    "\n",
    "\n",
    "### 4.\tWhen should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "The gRPC API is generally recommended when querying a model served by TF Serving because it is more efficient and faster than the REST API. The gRPC API uses protocol buffers to serialize data, which is a more compact and faster format than JSON. In addition, gRPC uses HTTP/2, which allows for multiplexing multiple requests and responses over a single connection, reducing latency and increasing throughput.\n",
    "\n",
    "\n",
    "### 5.\tWhat are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?\n",
    "TFLite reduces a model's size to make it run on mobile or embedded devices by using several techniques, including quantization, pruning, and compression. Quantization reduces the precision of the model's weights and activations from 32-bit floating-point numbers to 8-bit integers, which reduces the model's size and makes it faster to execute on devices with limited computing power. Pruning removes unimportant weights from the model, further reducing its size. Compression techniques such as Huffman coding and arithmetic coding can be used to further reduce the size of the model.\n",
    "\n",
    "\n",
    "### 6.\tWhat is quantization-aware training, and why would you need it?\n",
    "Quantization-aware training is a technique that involves training a model with the goal of making it more robust to the effects of quantization. When a model is quantized, the precision of its weights and activations is reduced, which can cause a loss in accuracy. Quantization-aware training involves adding special loss functions to the model that encourage the weights and activations to take values that are more amenable to quantization. This can help reduce the loss in accuracy that occurs when the model is quantized.\n",
    "\n",
    "\n",
    "### 7.\tWhat are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "Model parallelism and data parallelism are two techniques for distributing the training of a machine learning model across multiple machines. Model parallelism involves splitting the model across multiple machines, with each machine responsible for computing a portion of the model's output. Data parallelism involves replicating the model on multiple machines, with each machine processing a different subset of the training data. Data parallelism is generally recommended because it is easier to implement and more efficient, as it allows for larger batch sizes and reduces the communication overhead between machines.\n",
    "\n",
    "### 8.\tWhen training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
    "\n",
    "When training a model across multiple servers, you can use several distribution strategies, including MirroredStrategy, ParameterServerStrategy, and MultiWorkerMirroredStrategy. MirroredStrategy involves replicating the model on each machine and using synchronous updates to keep the models in sync. ParameterServerStrategy involves partitioning the model's variables across multiple servers and using asynchronous updates to update the variables. MultiWorkerMirroredStrategy is similar to MirroredStrategy but allows for distributed training across multiple machines. The choice of distribution strategy depends on the size of the model, the number of machines available, and the communication infrastructure between the machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
