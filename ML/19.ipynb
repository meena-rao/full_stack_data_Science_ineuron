{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf766dc",
   "metadata": {},
   "source": [
    "### ML 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f1aa4",
   "metadata": {},
   "source": [
    "### 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30. ? Using the k-means method, create two clusters for each set of centroid described above. For each set of centroid values, calculate the SSE.\n",
    "\n",
    "To create two clusters using k-means method, we follow the following steps:\n",
    "\n",
    "Initialize k centroids randomly.\n",
    "Assign each data point to the nearest centroid to form k clusters.\n",
    "Compute the SSE for each cluster.\n",
    "Update the centroid of each cluster by taking the mean of the data points in the cluster.\n",
    "Repeat steps 2 to 4 until the centroids do not change or a maximum number of iterations is reached.\n",
    "Using the given one-dimensional data points, we can create two clusters for each set of centroid as follows:\n",
    "\n",
    "For the first set of centroid (15, 32):\n",
    "\n",
    "Iteration 1:\n",
    "Assign data points 5, 10, 15, 20 to centroid 15 and data points 25, 30, 35 to centroid 32.\n",
    "Compute the SSE for each cluster: SSE1 = (5-15)^2 + (10-15)^2 + (15-15)^2 + (20-15)^2 = 200 and SSE2 = (25-32)^2 + (30-32)^2 + (35-32)^2 = 50.\n",
    "Update the centroid of each cluster: centroid 1 = (5+10+15+20)/4 = 12.5 and centroid 2 = (25+30+35)/3 = 30.\n",
    "\n",
    "Iteration 2:\n",
    "Assign data points 5, 10, 15 to centroid 12.5 and data points 20, 25, 30, 35 to centroid 30.\n",
    "Compute the SSE for each cluster: SSE1 = (5-12.5)^2 + (10-12.5)^2 + (15-12.5)^2 = 37.5 and SSE2 = (20-30)^2 + (25-30)^2 + (30-30)^2 + (35-30)^2 = 150.\n",
    "Update the centroid of each cluster: centroid 1 = (5+10+15)/3 = 10 and centroid 2 = (20+25+30+35)/4 = 27.5.\n",
    "\n",
    "Iteration 3:\n",
    "Assign data points 5, 10, 15 to centroid 10 and data points 20, 25, 30, 35 to centroid 27.5.\n",
    "Compute the SSE for each cluster: SSE1 = (5-10)^2 + (10-10)^2 + (15-10)^2 = 25 and SSE2 = (20-27.5)^2 + (25-27.5)^2 + (30-27.5)^2 + (35-27.5)^2 = 112.5.\n",
    "Update the centroid of each cluster: centroid 1 = (5+10+15)/3 = 10 and centroid 2 = (20+25+30+35)/4 = 27.5.\n",
    "The clusters obtained are {5, 10, 15} and {20, 25, 30, 35}. The SSE for the first set of centroid is SSE1 + SSE2 = 200 + 50 + 37.5 + 150 + 25 + 112.5 = 575.\n",
    "\n",
    "For the second set of centroid (12, 30):\n",
    "\n",
    "Iteration 1:\n",
    "Assign data points 5, 10, 15, 20 to centroid 12 and data points 25, 30, 35 to centroid 30.\n",
    "Compute the SSE for each cluster: SSE1 = (5-12)^2 + (10-12)^2 + (15-12)^2 + (20-12)^2 = 142 and SSE2 = (25-30)^2 + (30-30)^2 + (35-30)^2 = 34.\n",
    "\n",
    "Update the centroid of each cluster: centroid 1 = (5+10+15+20)/4 = 12.5 and centroid 2 = (25+30+35)/3 = 30.\n",
    "\n",
    "Iteration 2:\n",
    "Assign data points 5, 10, 15, 20 to centroid 12.5 and data points 25, 30, 35 to centroid 30.\n",
    "Compute the SSE for each cluster: SSE1 = (5-12.5)^2 + (10-12.5)^2 + (15-12.5)^2 + (20-12.5)^2 = 137.5 and SSE2 = (25-30)^2 + (30-30)^2 + (35-30)^2 = 25.\n",
    "\n",
    "Update the centroid of each cluster: centroid 1 = (5+10+15+20)/4 = 12.5 and centroid 2 = (25+30+35)/3 = 30.\n",
    "The clusters obtained are {5, 10, 15, 20} and {25, 30, 35}. The SSE for the second set of centroid is SSE1 + SSE2 = 142 + 34 + 137.5 + 25 = 338.5.\n",
    "\n",
    "Therefore, the SSE for the first set of centroid is 575 and the SSE for the second set of centroid is 338.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7810338",
   "metadata": {},
   "source": [
    "2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration.\n",
    "\n",
    "5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "6. At the start of the clustering exercise, discuss one method for determining the required number of clusters.\n",
    "\n",
    "7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "8. Draw a diagram to demonstrate the principle of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f3ed0",
   "metadata": {},
   "source": [
    "### Answers from 2 to 8\n",
    "2. Market Basket Analysis uses association analysis concepts to identify associations between products that are frequently purchased together. The concept of support, confidence, and lift are used to quantify the strength of these associations. Association rules are generated, and these rules are used to guide marketing strategies such as product placement, bundling, and targeted advertising.\n",
    "\n",
    "3. Apriori algorithm is a classic algorithm used for mining frequent itemsets and generating association rules. For example, consider a transaction database consisting of five transactions:\n",
    "\n",
    "T1: {milk, bread, eggs}\n",
    "T2: {milk, bread}\n",
    "T3: {milk, diapers}\n",
    "T4: {beer, diapers}\n",
    "T5: {beer, chips}\n",
    "\n",
    "The Apriori algorithm works by first finding all frequent 1-itemsets (items that appear in at least min_sup transactions). In this example, milk, bread, and diapers are frequent 1-itemsets. Then, it finds all frequent 2-itemsets (pairs of items that appear in at least min_sup transactions) using the frequent 1-itemsets. In this example, {milk, bread} and {milk, diapers} are frequent 2-itemsets. Finally, it generates association rules from the frequent itemsets and calculates their support, confidence, and lift.\n",
    "\n",
    "4. In hierarchical clustering, the distance between clusters is measured using a distance metric such as Euclidean distance or Manhattan distance. The distance between two clusters is calculated based on the distance between their constituent data points. The linkage criterion, such as single linkage, complete linkage, or average linkage, is used to determine how to calculate the distance between two clusters. The iteration ends when all data points are in the same cluster or when the distance between all clusters exceeds a predefined threshold.\n",
    "\n",
    "5. In the k-means algorithm, the cluster centroids are recomputed by taking the mean of all data points assigned to each cluster. Specifically, for each cluster, the new centroid is computed as the mean of all data points assigned to that cluster.\n",
    "\n",
    "6. One method for determining the required number of clusters is the elbow method. In this method, the SSE is computed for a range of cluster numbers, and the results are plotted on a graph. The elbow point on the graph indicates the number of clusters where the SSE begins to level off, indicating that additional clusters would not lead to significant improvements in clustering performance.\n",
    "\n",
    "7. Advantages of the k-means algorithm include its simplicity, speed, and effectiveness for clustering large datasets. However, it can be sensitive to the initial choice of centroids and may converge to local optima. It also assumes that the clusters are spherical and have equal variance, which may not always be the case in real-world datasets.\n",
    "\n",
    "8. The principle of clustering can be demonstrated with a diagram where data points are grouped into clusters based on their similarities. In a simple 2D space, data points can be plotted on an x-y axis, and clusters can be represented by different colors or shapes. The clustering algorithm works by assigning data points to clusters based on their proximity, and the clusters are iteratively refined until convergence is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dfeb2",
   "metadata": {},
   "source": [
    "9. 9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering&#39;s SSE be?\n",
    "\n",
    "### Answer\n",
    "It seems that there might be some typos in the given data points, as the cluster assignments for C1 and C2 are the same. Assuming that C1 and C2 are meant to be different clusters, we can proceed with the problem as follows:\n",
    "\n",
    "First iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,0)\n",
    "C3: (5,5), (9,9)\n",
    "\n",
    "The initial centroids are:\n",
    "Centroid1: (4,4)\n",
    "Centroid2: (0,2)\n",
    "Centroid3: (7,7)\n",
    "\n",
    "After the first iteration, the cluster assignments and new centroids are:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,0)\n",
    "C3: (5,5), (9,9)\n",
    "\n",
    "New Centroid1: (4,4)\n",
    "New Centroid2: (0,2.67)\n",
    "New Centroid3: (7,7)\n",
    "\n",
    "Second iteration:\n",
    "\n",
    "After the second iteration, the cluster assignments and new centroids are:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,0), (5,5)\n",
    "C3: (9,9)\n",
    "\n",
    "New Centroid1: (4,4)\n",
    "New Centroid2: (1,2.8)\n",
    "New Centroid3: (9,9)\n",
    "\n",
    "The clustering's SSE can be calculated as follows:\n",
    "\n",
    "SSE = sum of squared errors within each cluster\n",
    "SSE = sum of distances between each point and its centroid, squared\n",
    "\n",
    "SSE = [(2-4)^2 + (4-4)^2 + (6-4)^2] + [(0-1)^2 + (4-2.67)^2 + (0-1)^2 + (0-1)^2 + (0-1)^2 + (0-1)^2 + (0-1)^2 + (0-1)^2 + (0-0)^2 + (5-7)^2] + [(9-9)^2]\n",
    "SSE = 4 + 32.42 + 0\n",
    "SSE = 36.42\n",
    "\n",
    "Therefore, the cluster centroids after the second iteration are (4,4), (1,2.8), and (9,9), and the SSE for this clustering is 36.42."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6df95",
   "metadata": {},
   "source": [
    "10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm.\n",
    "\n",
    "### Answer\n",
    "Collect the defect data points and preprocess them to extract relevant features and convert them into a numerical format.\n",
    "\n",
    "Apply the k-means algorithm to cluster the defects into 5 groups based on their similarities. The algorithm will assign each data point to the closest centroid, and then update the centroids based on the mean of the points in each cluster.\n",
    "\n",
    "Once the clustering is complete, any new defects that are discovered can be assigned to one of the 5 clusters based on their similarities to the existing clusters. This can be done by calculating the distance between the new defect and the centroids of each cluster, and assigning it to the closest one.\n",
    "\n",
    "The process can be repeated periodically as new defects are discovered, allowing the team to continually update the clustering and improve the accuracy of their defect categorization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
