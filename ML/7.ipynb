{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a95f7e",
   "metadata": {},
   "source": [
    "## ML7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08be898",
   "metadata": {},
   "source": [
    "### 1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?\n",
    "In machine learning, a target function (also known as the objective function) is a function that is used to map input variables to output variables. The goal of the machine learning algorithm is to learn this target function from a set of labeled training data so that it can make accurate predictions on new, unseen data.\n",
    "\n",
    "A real-life example of a target function could be predicting the price of a house based on its size, location, and other features. The target function would take in these input variables and output a predicted price for the house.\n",
    "\n",
    "The fitness of a target function is assessed by measuring its performance on a set of evaluation metrics. These metrics depend on the specific task at hand, but common examples include accuracy, precision, recall, F1 score, mean squared error, and mean absolute error. The machine learning algorithm will try to optimize the target function by adjusting its parameters until it achieves the best possible performance on these metrics.\n",
    "\n",
    "### 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models.\n",
    "Predictive models are machine learning models that use statistical algorithms and historical data to make predictions about future outcomes. These models learn from past data to identify patterns and relationships that can be used to predict future events. They are used to make predictions about a wide range of phenomena, from the stock market to weather patterns to customer behavior.\n",
    "\n",
    "__Predictive models__ work by using historical data to identify patterns and relationships that can be used to make predictions about future events. The model will analyze the data and create a mathematical function that can be used to predict the outcome of a particular event based on a set of input variables. This function can then be used to make predictions about future events by inputting new data into the model.\n",
    "\n",
    "An example of a predictive model is a credit scoring model used by banks and other financial institutions to assess the creditworthiness of loan applicants. The model will analyze past data on loan applications and credit histories to identify patterns and relationships that can be used to predict the likelihood of a loan default. The model will then use this information to assign a credit score to each loan applicant, which is used to determine whether or not they are approved for a loan.\n",
    "\n",
    "__Descriptive models,__ on the other hand, are used to describe patterns and relationships in historical data. They are used to gain insights into past events and understand the underlying factors that contributed to those events. Descriptive models do not make predictions about future events but rather describe what has happened in the past.\n",
    "\n",
    "An example of a descriptive model is a demographic analysis of a city's population. The model would analyze historical data on the age, gender, income, and other demographic factors of the city's residents to identify patterns and relationships in the data. This information can be used to understand the underlying factors that have contributed to changes in the city's population over time.\n",
    "\n",
    "The main difference between predictive and descriptive models is that predictive models are used to make predictions about future events, while descriptive models are used to gain insights into past events. Both types of models can be used to inform decision-making in different fields, including business, finance, healthcare, and government\n",
    "\n",
    "### 3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters.\n",
    "When evaluating the performance of a classification model, there are several measurement parameters that are commonly used:\n",
    "\n",
    "__Confusion Matrix:__ A confusion matrix is a table that is used to evaluate the performance of a classification model. It shows the number of true positive, true negative, false positive, and false negative predictions made by the model.\n",
    "\n",
    "__Accuracy:__ Accuracy is the most basic measurement parameter that measures the percentage of correct predictions made by the model out of all the predictions made.\n",
    "\n",
    "__Precision: Precision__ is the measurement parameter that measures the proportion of true positives out of all the positive predictions made by the model.\n",
    "\n",
    "__Recall (Sensitivity): __Recall is the measurement parameter that measures the proportion of true positives out of all the actual positive instances in the dataset.\n",
    "\n",
    "__F1 Score:__ F1 score is the harmonic mean of precision and recall, and it is used to balance between the two measures.\n",
    "\n",
    "__Specificity:__ Specificity measures the proportion of true negatives out of all the actual negative instances in the dataset.\n",
    "\n",
    "__Receiver Operating Characteristic (ROC) Curve:__ ROC curve is a graphical representation of the performance of a classification model. It plots the true positive rate against the false positive rate at different probability thresholds.\n",
    "\n",
    "__Area Under Curve (AUC):__ AUC is the area under the ROC curve and measures the overall performance of the model.\n",
    "\n",
    "To assess the efficiency of a classification model, one can use any or all of these measurement parameters. The choice of which parameters to use depends on the specific use case and the objectives of the model. Generally, a good model is one that has a high accuracy, precision, recall, F1 score, and AUC, and low false positive and false negative rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c6453",
   "metadata": {},
   "source": [
    "### 4. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting:\n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "### What does it mean to overfit? When is it going to happen?\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.\n",
    "\n",
    "### In the sense of model fitting, explain the bias-variance trade-off\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set.\n",
    "The bias-variance trade-off is a fundamental concept in machine learning and model fitting. It refers to the balance between two types of errors that can occur in a model: bias error and variance error.\n",
    "\n",
    "__Bias error__ occurs when a model is unable to capture the true underlying relationship between the input and output variables, resulting in a high degree of error even on the training data. This is often caused by models that are too simple or that make strong assumptions about the data.\n",
    "\n",
    "__Variance error__, on the other hand, occurs when a model is too complex and is overfitting to the noise in the data. This results in a low error on the training data, but a high error on the testing data because the model is unable to generalize to new data.\n",
    "\n",
    "The bias-variance trade-off states that as a model becomes more complex, its variance error will decrease, but its bias error will increase. Conversely, as a model becomes less complex, its bias error will decrease, but its variance error will increase. The goal is to find the optimal balance between bias and variance that results in the lowest possible overall error.\n",
    "\n",
    "To achieve this balance, one can use techniques such as cross-validation and regularization. Cross-validation can help identify the optimal model complexity by assessing the performance of the model on independent datasets. Regularization can help reduce variance by introducing a penalty term for large model parameters, effectively limiting the model's complexity.\n",
    "\n",
    "In summary, the bias-variance trade-off is the balance between the two types of errors that can occur in a model: bias error and variance error. The goal is to find the optimal balance between bias and variance that results in the lowest possible overall error, which can be achieved through techniques such as cross-validation and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d678508",
   "metadata": {},
   "source": [
    "### 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n",
    "Yes, it is possible to boost the efficiency of a learning model using several techniques. Here are some ways to do so:\n",
    "\n",
    "__Feature Selection:__ Feature selection is the process of selecting the most relevant features from the dataset to reduce the dimensionality of the problem. This can improve the efficiency of the learning model by reducing overfitting, improving model interpretability, and reducing computational complexity.\n",
    "\n",
    "__Hyperparameter Tuning:__ Hyperparameter tuning is the process of selecting the optimal hyperparameters for the learning model. This can improve the efficiency of the model by finding the best configuration of the model to maximize performance on the validation set.\n",
    "\n",
    "__Regularization:__ Regularization is the process of adding a penalty term to the objective function to prevent overfitting. This can improve the efficiency of the model by reducing variance and increasing generalization ability.\n",
    "\n",
    "__Ensembling:__ Ensembling is the process of combining multiple models to improve the accuracy and efficiency of the learning model. This can be done by using techniques such as bagging, boosting, and stacking.\n",
    "\n",
    "__Transfer Learning:__ Transfer learning is the process of leveraging pre-trained models on a related task to improve the efficiency of the learning model. This can reduce the amount of training required and improve generalization ability.\n",
    "\n",
    "__Data Augmentation:__ Data augmentation is the process of generating synthetic data from existing data to increase the size of the training set. This can improve the efficiency of the learning model by reducing overfitting and increasing the diversity of the training data.\n",
    "\n",
    "In summary, there are several techniques that can be used to boost the efficiency of a learning model, including feature selection, hyperparameter tuning, regularization, ensembling, transfer learning, and data augmentation. The choice of technique depends on the specific use case and the objectives of the model.\n",
    "\n",
    "### 6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?\n",
    "The success of an unsupervised learning model is typically assessed using various evaluation metrics that measure the quality of the clustering or dimensionality reduction produced by the model. Here are some of the most common success indicators for an unsupervised learning model:\n",
    "\n",
    "__Silhouette Score:__ The silhouette score measures the quality of the clustering produced by the model by computing the mean distance between each sample and all other samples in the same cluster, as well as the mean distance between each sample and all other samples in the nearest neighboring cluster. A higher silhouette score indicates that the samples are well-clustered and distinct from samples in other clusters.\n",
    "\n",
    "__Calinski-Harabasz Index:__ The Calinski-Harabasz index measures the quality of the clustering by computing the ratio of the between-cluster dispersion to the within-cluster dispersion. A higher Calinski-Harabasz index indicates that the clusters are well-separated and distinct from each other.\n",
    "\n",
    "__Davies-Bouldin Index:__ The Davies-Bouldin index measures the quality of the clustering by computing the ratio of the within-cluster dispersion to the between-cluster dispersion. A lower Davies-Bouldin index indicates that the clusters are well-separated and distinct from each other.\n",
    "\n",
    "__Explained Variance Ratio:__ The explained variance ratio measures the amount of variance in the original data that is explained by the reduced dimensional representation produced by the model. A higher explained variance ratio indicates that the reduced dimensional representation retains more of the important features of the original data.\n",
    "\n",
    "__Reconstruction Error:__ The reconstruction error measures the quality of the dimensionality reduction produced by the model by computing the difference between the original data and the reconstructed data from the reduced dimensional representation. A lower reconstruction error indicates that the reduced dimensional representation retains more of the important features of the original data.\n",
    "\n",
    "In summary, the success of an unsupervised learning model is typically evaluated using various metrics that measure the quality of the clustering or dimensionality reduction produced by the model, such as the silhouette score, Calinski-Harabasz index, Davies-Bouldin index, explained variance ratio, and reconstruction error. The choice of evaluation metric depends on the specific use case and the objectives of the model.\n",
    "\n",
    "### 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n",
    "It is not recommended to use a classification model for numerical data or a regression model for categorical data. This is because classification models and regression models are designed to handle different types of data and have different output structures.\n",
    "\n",
    "__Classification models__ are used to predict categorical outcomes, where the output variable is a discrete set of categories or labels. The goal is to assign each observation to one of the pre-defined categories. Examples of classification models include logistic regression, decision trees, and support vector machines. These models are not designed to handle numerical data as input, as they require categorical input features.\n",
    "\n",
    "__Regression models__, on the other hand, are used to predict continuous outcomes, where the output variable is a numerical value. The goal is to estimate the relationship between the input features and the output variable. Examples of regression models include linear regression, polynomial regression, and random forest regression. These models are not designed to handle categorical data as input, as they require numerical input features.\n",
    "\n",
    "Therefore, it is important to choose the appropriate model type for the type of data being used. If the input data is numerical, a regression model should be used, and if the input data is categorical, a classification model should be used. Attempting to use the wrong type of model can lead to inaccurate predictions and poor model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92c0a5",
   "metadata": {},
   "source": [
    "### 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n",
    "Predictive modeling for numerical values, also known as regression modeling, involves building a model to predict a continuous numerical output variable based on a set of input variables. The goal is to estimate the relationship between the input variables and the output variable in order to make predictions for new observations.\n",
    "\n",
    "The process of building a predictive model for numerical values involves several steps, including:\n",
    "\n",
    "Data preprocessing: This involves cleaning and preparing the data for modeling, including handling missing values, scaling the features, and handling outliers.\n",
    "\n",
    "Feature selection: This involves identifying the most relevant features for predicting the output variable and removing any irrelevant features.\n",
    "\n",
    "Model selection: This involves selecting an appropriate regression model, such as linear regression, polynomial regression, or random forest regression, based on the characteristics of the data and the research question.\n",
    "\n",
    "Model training: This involves using the training data to fit the chosen regression model and estimate the model parameters.\n",
    "\n",
    "Model evaluation: This involves evaluating the performance of the model using appropriate evaluation metrics, such as mean squared error, mean absolute error, and R-squared.\n",
    "\n",
    "In contrast, predictive modeling for categorical values, also known as classification modeling, involves building a model to predict a categorical output variable based on a set of input variables. The goal is to assign each observation to one of several pre-defined categories.\n",
    "\n",
    "The process of building a predictive model for categorical values is similar to that of regression modeling, but with some differences. For example, the choice of model and evaluation metrics may differ, and feature selection may be more complex due to the nature of categorical variables.\n",
    "\n",
    "Overall, the main distinction between predictive modeling for numerical values and categorical values is the type of output variable being predicted. Numerical modeling involves predicting a continuous numerical output, while categorical modeling involves predicting a discrete categorical output.\n",
    "\n",
    "### 9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n",
    "i. Accurate estimates – 15 cancerous, 75 benign\n",
    "ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n",
    "\n",
    "__ANS:__\n",
    "Based on the given information, we can calculate the various evaluation metrics as follows:\n",
    "\n",
    "True positives (TP): 15\n",
    "False positives (FP): 7\n",
    "False negatives (FN): 3\n",
    "True negatives (TN): 75\n",
    "__Error rate:__\n",
    "The error rate is the proportion of incorrect predictions made by the model. It is calculated as:\n",
    "\n",
    "Error rate = (FP + FN) / (TP + FP + FN + TN) = (7 + 3) / (15 + 7 + 3 + 75) = 0.08 or 8%\n",
    "\n",
    "__Kappa value:__\n",
    "The Kappa value is a measure of agreement between the predicted and actual classifications, adjusted for chance agreement. It is calculated as:\n",
    "\n",
    "Kappa = (accuracy - expected accuracy) / (1 - expected accuracy)\n",
    "where accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "and expected accuracy = ((TP + FP) * (TP + FN) + (TN + FP) * (TN + FN)) / (TP + FP + FN + TN)²\n",
    "\n",
    "Substituting the values, we get:\n",
    "\n",
    "accuracy = (15 + 75) / (15 + 7 + 3 + 75) = 0.9\n",
    "expected accuracy = ((15 + 7) * (15 + 3) + (75 + 7) * (75 + 3)) / (15 + 7 + 3 + 75)² = 0.786\n",
    "\n",
    "Kappa = (0.9 - 0.786) / (1 - 0.786) = 0.364\n",
    "\n",
    "__Sensitivity:__\n",
    "Sensitivity is the proportion of actual positive cases that were correctly identified by the model. It is calculated as:\n",
    "\n",
    "Sensitivity = TP / (TP + FN) = 15 / (15 + 3) = 0.833\n",
    "\n",
    "__Precision:_ \n",
    "Precision is the proportion of positive predictions that were correct. It is calculated as:\n",
    "\n",
    "Precision = TP / (TP + FP) = 15 / (15 + 7) = 0.682\n",
    "\n",
    "__F-measure:__\n",
    "The F-measure is the harmonic mean of precision and sensitivity, and provides a balanced measure of both metrics. It is calculated as:\n",
    "\n",
    "F-measure = 2 * Precision * Sensitivity / (Precision + Sensitivity) = 2 * 0.682 * 0.833 / (0.682 + 0.833) = 0.750\n",
    "\n",
    "Therefore, the error rate is 8%, the Kappa value is 0.364, the sensitivity is 0.833, the precision is 0.682, and the F-measure is 0.750 for the given classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713f4ad",
   "metadata": {},
   "source": [
    "### 10. Make quick notes on: \n",
    "### 1. The process of holding out\n",
    "The process of holding out, also known as holdout validation, is a method used to evaluate the performance of a machine learning model on new and unseen data. The purpose of holding out is to estimate how well the model will perform on data that it has not yet been trained on, and to detect if there is overfitting or underfitting in the model.\n",
    "\n",
    "In the holdout method, a portion of the available dataset is set aside as a validation dataset, while the rest is used as the training dataset. The training dataset is used to train the machine learning model, while the validation dataset is used to evaluate its performance. The validation dataset is typically chosen randomly from the available data, with the size of the validation set being determined by the size of the available data and the desired level of statistical significance.\n",
    "\n",
    "The holdout process can be performed using various techniques such as simple random sampling, stratified sampling, or k-fold cross-validation. In simple random sampling, a random subset of the data is selected as the validation dataset. Stratified sampling ensures that the distribution of classes in the validation dataset is similar to that in the training dataset. In k-fold cross-validation, the data is split into k folds, with each fold used as the validation set in turn, and the remaining data used for training.\n",
    "\n",
    "Once the holdout process is complete, the model's performance is evaluated on the validation dataset using metrics such as accuracy, precision, recall, F1-score, and others. The model's performance on the validation set is then used to make decisions about model selection, parameter tuning, and further improvements.\n",
    "\n",
    "In summary, holding out is a critical step in the machine learning workflow that allows for the estimation of a model's performance on new and unseen data, which is important for the development of accurate and reliable machine learning models.\n",
    "\n",
    "### 2. Cross-validation by tenfold\n",
    "Cross-validation by tenfold, also known as 10-fold cross-validation, is a popular method used to evaluate the performance of a machine learning model. It involves splitting the available dataset into ten equal parts or folds. Nine of these folds are used as the training dataset, while the remaining fold is used as the validation dataset.\n",
    "\n",
    "The model is trained on the nine folds, and its performance is evaluated on the remaining fold. This process is repeated ten times, with each fold being used once as the validation dataset. The results of each iteration are averaged to obtain a more robust estimate of the model's performance.\n",
    "\n",
    "The main advantage of 10-fold cross-validation is that it provides a more reliable estimate of a model's performance than a simple holdout method, where only one training-validation split is made. This is because 10-fold cross-validation uses multiple training-validation splits, which reduces the risk of bias and variance.\n",
    "\n",
    "Moreover, 10-fold cross-validation is computationally efficient as it requires training the model ten times only, unlike leave-one-out cross-validation, where the model is trained as many times as there are data points in the dataset.\n",
    "\n",
    "In summary, cross-validation by tenfold is a powerful method to evaluate the performance of a machine learning model. It allows for a more robust estimate of a model's performance and reduces the risk of bias and variance.\n",
    "\n",
    "### 3. Adjusting the parameters\n",
    "In machine learning, adjusting the parameters of a model is an important step in improving its performance. The process of adjusting the parameters is often called parameter tuning or hyperparameter optimization.\n",
    "\n",
    "Parameters in a machine learning model are values that the algorithm estimates during the training process to make predictions on new data. Hyperparameters, on the other hand, are settings that determine how the algorithm learns the parameters. Examples of hyperparameters include the learning rate in gradient descent algorithms, the number of hidden layers and neurons in neural networks, and the number of trees in a random forest algorithm.\n",
    "\n",
    "To adjust the parameters of a model, we need to find the best set of hyperparameters that optimize the model's performance. This involves selecting a range of hyperparameters and testing the model's performance with each set. The performance of the model can be evaluated using a validation set or through cross-validation.\n",
    "\n",
    "There are several approaches to parameter tuning, including grid search, random search, and Bayesian optimization. Grid search involves testing all possible combinations of hyperparameters within a predefined range. Random search involves sampling hyperparameters randomly from a defined distribution. Bayesian optimization uses a probabilistic model to choose the best hyperparameters to test.\n",
    "\n",
    "Once the optimal hyperparameters are found, the model can be trained on the entire dataset with the best hyperparameters. This should result in a more accurate and efficient model.\n",
    "\n",
    "In summary, adjusting the parameters of a machine learning model is an essential step in improving its performance. It involves selecting a range of hyperparameters, testing the model's performance with each set, and finding the optimal hyperparameters that optimize the model's performance.\n",
    "\n",
    "### 11. Define the following terms:\n",
    "### 1. Purity vs. Silhouette width\n",
    "Purity and silhouette width are two common evaluation metrics used in clustering analysis. They are both used to measure the quality of the clusters obtained by a clustering algorithm, but they differ in their approach to measuring cluster quality.\n",
    "\n",
    "Purity is a measure of how well-defined the clusters are in terms of their assigned class labels. It is calculated as the proportion of data points in a cluster that belong to the majority class of that cluster. Higher purity values indicate that the clusters are more distinct in terms of their class labels. However, purity does not take into account the internal structure of the clusters, and clusters with high purity may still have overlapping or poorly separated boundaries.\n",
    "\n",
    "On the other hand, silhouette width is a measure of how well-separated the clusters are in terms of their internal structure. It is calculated as the difference between the mean distance of a data point to all other points in its own cluster and the mean distance of that point to all points in the nearest neighboring cluster, divided by the maximum of these two values. Higher silhouette width values indicate that the clusters are well-separated and distinct from each other. Silhouette width takes into account the internal structure of the clusters and can identify clusters with overlapping or poorly separated boundaries.\n",
    "\n",
    "In summary, purity measures how well-defined the clusters are in terms of their assigned class labels, while silhouette width measures how well-separated the clusters are in terms of their internal structure. Both metrics have their strengths and weaknesses, and the choice of metric depends on the specific clustering problem and the goals of the analysis.\n",
    "\n",
    "### 2. Boosting vs. Bagging\n",
    "Boosting and bagging are two popular ensemble learning methods used to improve the performance of machine learning models.\n",
    "\n",
    "Bagging, which stands for bootstrap aggregating, involves training multiple independent models on randomly sampled subsets of the training data, and then combining the predictions of these models to obtain a final prediction. Bagging is effective in reducing the variance of the model and improving its generalization performance, especially for unstable models such as decision trees. Examples of bagging algorithms include random forest and extra trees.\n",
    "\n",
    "Boosting, on the other hand, is an iterative method that involves training multiple models sequentially, with each subsequent model attempting to correct the errors of the previous models. Boosting is effective in reducing the bias of the model and improving its accuracy, especially for weak models such as decision stumps. Examples of boosting algorithms include AdaBoost, gradient boosting, and XGBoost.\n",
    "\n",
    "The main difference between bagging and boosting is in the way the models are combined. In bagging, the models are combined in a parallel and independent manner, while in boosting, the models are combined in a sequential and dependent manner. This difference also affects the types of models that are suitable for each method. Bagging is effective for reducing variance, while boosting is effective for reducing bias.\n",
    "\n",
    "In summary, bagging and boosting are two popular ensemble learning methods that can improve the performance of machine learning models. Bagging reduces variance by training multiple independent models, while boosting reduces bias by training multiple models sequentially. The choice of method depends on the specific machine learning problem and the characteristics of the data and model.\n",
    "\n",
    "### 3. The eager learner vs. the lazy learner\n",
    "The eager learner and the lazy learner are two different types of machine learning algorithms based on their approach to learning from data.\n",
    "\n",
    "__The eager learner,__ also known as the eager or eager learning algorithm, is a type of algorithm that eagerly learns a model from the training data before being presented with new, unseen data. This means that the eager learner builds a model that summarizes the training data as a generalization for future predictions. Examples of eager learning algorithms include decision trees, neural networks, and logistic regression. Eager learners are characterized by fast training times, but they may overfit to the training data and generalize poorly to new, unseen data.\n",
    "\n",
    "__The lazy learner,__ also known as the instance-based learner or lazy learning algorithm, is a type of algorithm that postpones the model building until it receives new, unseen data. This means that the lazy learner stores the entire training data set and uses it to make predictions on new data points by comparing the similarity between the new data and the training data. Examples of lazy learning algorithms include k-nearest neighbors (KNN) and case-based reasoning. Lazy learners are characterized by slow training times, but they can be more accurate and generalize better to new data than eager learners, especially when dealing with noisy or complex data.\n",
    "\n",
    "In summary, the main difference between the eager learner and the lazy learner is in the timing of the model building. The eager learner builds a model before seeing new data, while the lazy learner postpones the model building until new data arrives. The choice of learner depends on the specific machine learning problem, the nature of the data, and the desired performance criteria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
