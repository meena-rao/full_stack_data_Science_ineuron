{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46655fb",
   "metadata": {},
   "source": [
    "### ML5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c104bc4",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks that machine learning entails? What does data pre-processing imply?\n",
    "There are Five core tasks in the common ML workflow:\n",
    "\n",
    "__Get Data:__ The first step in the Machine Learning process is getting data.\n",
    "__Cleaning, Preparing & Manipulating Data:__ Real-world data often has unorganized, missing, or noisy elements.\n",
    "__Train Model:__ This step is where the magic happens!\n",
    "__Testing Model.__\n",
    "__Improving model.__\n",
    "__Data preprocessing__ involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "Preprocessing involves both data validation and data imputation\n",
    "__The Goal of Data Validation__ is to assess whether the data in question is both complete and accurate.\n",
    "__The Goal of Data Imputation__ is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming.\n",
    "\n",
    "### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two.\n",
    "1.Quantitative\n",
    "2. Qualitative\n",
    "__Quantitative Data Type:__ This Type Of Data Type Consists Of Numerical Values. Anything Which Is Measured By Numbers. E.G., Profit, Quantity Sold, Height, Weight, Temperature, Etc. This Is Again Of Two Types\n",
    "\n",
    "__Discrete Data Type:__ – The Numeric Data Which Have Discrete Values Or Whole Numbers. This Type Of Variable Value If Expressed In Decimal Format Will Have No Proper Meaning. Their Values Can Be Counted. E.G.: – No. Of Cars You Have, No. Of Marbles In Containers, Students In A Class, Etc.\n",
    "\n",
    "__Continuous Data Type:__ – The Numerical Measures Which Can Take The Value Within A Certain Range. This Type Of Variable Value If Expressed In Decimal Format Has True Meaning. Their Values Can Not Be Counted But Measured. The Value Can Be Infinite E.G.: Height, Weight, Time, Area, Distance, Measurement Of Rainfall, Etc.\n",
    "\n",
    "__Qualitative Data Type:__ These Are The Data Types That Cannot Be Expressed In Numbers. This Describes Categories Or Groups And Is Hence Known As The Categorical Data Type. This Can Be Divided Into:-\n",
    "\n",
    "__Structured Data:__ This Type Of Data Is Either Number Or Words. This Can Take Numerical Values But Mathematical Operations Cannot Be Performed On It. This Type Of Data Is Expressed In Tabular Format. E.G.) Sunny=1, Cloudy=2, Windy=3 Or Binary Form Data Like 0 Or1, Good Or Bad, Etc.\n",
    "\n",
    "__Unstructured Data:__ This Type Of Data Does Not Have The Proper Format And Therefore Known As Unstructured Data.This Comprises Textual Data, Sounds, Images, Videos, Etc.\n",
    "\n",
    "### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types.\n",
    "__Determine What Information You Want to Collect__: The first thing you need to do is choose what details you want to collect. You’ll need to decide what topics the information will cover, who you want to collect it from and how much data you need. Your goals — what you hope to accomplish using your data — will determine your answers to these questions. As an example, you may decide to collect data about which type of articles are most popular on your website among visitors who are between the ages of 18 and 34. You might also choose to gather information about the average age of all of the customers who bought a product from your company within the last month.\n",
    "\n",
    "__Set a Timeframe for Data Collection:__ Next, you can start formulating your plan for how you’ll collect your data. In the early stages of your planning process, you should establish a timeframe for your data collection. You may want to gather some types of data continuously. When it comes to transactional data and website visitor data, for example, you may want to set up a method for tracking that data over the long term. If you’re tracking data for a specific campaign, however, you’ll track it over a defined period. In these instances, you’ll have a schedule for when you’ll start and end your data collection.\n",
    "\n",
    "__Determine Your Data Collection Method:__ At this step, you will choose the data collection method that will make up the core of your data-gathering strategy. To select the right collection method, you’ll need to consider the type of information you want to collect, the timeframe over which you’ll obtain it and the other aspects you determined.\n",
    "\n",
    "__Collect the Data:__ Once you have finalized your plan, you can implement your data collection strategy and start collecting data. You can store and organize your data in your DMP. Be sure to stick to your plan and check on its progress regularly. It may be useful to create a schedule for when you will check in with how your data collection is proceeding, especially if you are collecting data continuously. You may want to make updates to your plan as conditions change and you get new information.\n",
    "\n",
    "__Analyze the Data and Implement Your Findings:__ Once you’ve collected all of your data, it’s time to analyze it and organize your findings. The analysis phase is crucial because it turns raw data into valuable insights that you can use to enhance your marketing strategies, products and business decisions. You can use the analytics tools built into our DMP to help with this step. Once you’ve uncovered the patterns and insights in your data, you can implement the findings to improve your business\n",
    "\n",
    "### 4. What are the various causes of machine learning data issues? What are the ramifications?\n",
    "There are various causes of data issues in machine learning, some of which include:\n",
    "\n",
    "__Data quality issues:__ This can occur due to errors or inconsistencies in the data. For example, data may be missing, duplicated, or inaccurate.\n",
    "\n",
    "__Data bias:__ This refers to systematic errors in the data that can cause certain groups or characteristics to be overrepresented or underrepresented in the dataset. This can lead to biased predictions or models.\n",
    "\n",
    "__Data collection issues:__ This occurs when the data collected is not representative of the real-world population or is collected in a biased manner.\n",
    "\n",
    "__Data preprocessing errors:__ This can occur when the data is not properly cleaned, scaled, or normalized before being used for training a model.\n",
    "\n",
    "__Lack of data:__ This can be a major issue when working with machine learning models. If the dataset is too small, the model may not be able to learn the patterns and relationships in the data effectively.\n",
    "\n",
    "The ramifications of these data issues can be significant. For example, biased models can lead to discriminatory outcomes, and inaccurate models can lead to incorrect predictions or decisions. These issues can also affect the credibility and trustworthiness of machine learning models, which can impact their adoption and effectiveness. Additionally, it can result in wasted resources and time, as well as damage to the reputation of the organization.\n",
    "\n",
    "### 5. Demonstrate various approaches to categorical data exploration with appropriate examples.\n",
    "__1.Unique value count:__ One of the first things which can be useful during data exploration is to see how many unique values\n",
    "are there in categorical columns.\n",
    "\n",
    "__2. Frequency Count:__ Frequency count is finding how frequent individual values occur in column. \n",
    "\n",
    "__3. Variance:__ Variance gives a good indication how the values are spread.\n",
    "\n",
    "__4. Pareto Analysis:__ Pareto analysis is a creative way of focusing on what is important. Pareto 80–20 rule can be effectively used in data exploration. \n",
    "\n",
    "__5. Histogram:__ Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data. \n",
    "\n",
    "__6. Correlation Heat-map between all numeric columns:__ The term correlation refers to a mutual relationship or association between two things. \n",
    "\n",
    "__7. Pearson Correlation and Trend between two numeric columns:__ Once you have visualised correlation heat-map , the next step is to see the correlation trend between two specific numeric columns. \n",
    "\n",
    "__8. Outlier overview:__ Finding something unusual in data is called Outlier detection (also known as anomaly detection). These outliers represent something unusual, rare , anomaly or something exceptional.\n",
    "\n",
    "### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?\n",
    "If certain variables have missing values, the learning activity of the machine learning algorithm can be affected in several ways:\n",
    "\n",
    "__Biased results:__ The presence of missing values can introduce bias into the model since the data points with missing values may be treated differently than the complete data points.\n",
    "\n",
    "__Reduced accuracy:__ If the amount of missing data is significant, it can reduce the accuracy of the model, since the model is not being trained on the full dataset.\n",
    "\n",
    "__Overfitting:__ If the missing values are not handled properly, it can lead to overfitting, where the model becomes too complex and fits the training data too closely, resulting in poor generalization to new data.\n",
    "\n",
    "To address the issue of missing data, there are several strategies that can be employed:\n",
    "\n",
    "__Complete case analysis:_ This involves excluding all observations that have missing values. This approach is simple, but it can result in a loss of data and may introduce bias if the missing data is not missing at random.\n",
    "\n",
    "__Imputation:__ This involves filling in the missing values with estimated values. There are several methods for imputing missing data, including mean imputation, median imputation, and regression imputation. Imputation can help to preserve the integrity of the dataset while reducing bias.\n",
    "\n",
    "__Multiple imputation:__ This involves creating multiple imputed datasets and analyzing each dataset separately before combining the results. Multiple imputation is a more sophisticated approach than simple imputation and can help to address issues of uncertainty around missing data.\n",
    "\n",
    "In summary, missing data can have a significant impact on the performance of machine learning algorithms. To address this issue, researchers and practitioners should carefully consider the implications of missing data and choose an appropriate strategy to handle it.\n",
    "\n",
    "### 7. Describe the various methods for dealing with missing data values in depth.\n",
    "Dealing with missing data values is a common challenge in machine learning, and there are several methods that can be used to handle this issue. Here, I will describe some of the most commonly used methods for dealing with missing data in depth:\n",
    "\n",
    "#### Complete Case Analysis:\n",
    "Complete case analysis involves excluding all observations that have missing values from the dataset. This method is simple and straightforward, but it can result in a loss of data, which may lead to biased results if the missing data is not missing completely at random. In addition, this method may not be suitable if a large percentage of observations have missing data, as it can result in a significant reduction in the sample size.\n",
    "\n",
    "#### Mean/Median/Mode Imputation:\n",
    "This method involves filling in the missing values with the mean, median or mode of the corresponding variable. This method is simple and easy to implement, but it assumes that the missing values are missing at random and can lead to biased results if the missing data is not missing at random. Moreover, this method can reduce the variability of the data, as it replaces the missing values with a single value.\n",
    "\n",
    "#### Regression Imputation:\n",
    "Regression imputation involves filling in the missing values with predicted values based on a regression model. In this method, a regression model is first fitted to the complete cases of the data, and then the missing values are predicted using the regression model. This method is more complex than mean/median/mode imputation, but it can be more accurate if the regression model is a good fit for the data. However, this method assumes that the relationship between the predictor variables and the missing values is linear, which may not always be the case.\n",
    "\n",
    "#### Multiple Imputation:\n",
    "Multiple imputation is a more sophisticated method that involves creating multiple imputed datasets and analyzing each dataset separately before combining the results. In this method, missing values are imputed multiple times, resulting in multiple completed datasets. The analysis is then performed separately on each completed dataset, and the results are combined using a specified rule, such as averaging or majority voting. Multiple imputation can provide more accurate estimates than single imputation methods, as it accounts for the uncertainty around the imputed values. However, multiple imputation can be computationally intensive and requires careful consideration of the number of imputations and the imputation model.\n",
    "\n",
    "#### Machine Learning-Based Methods:\n",
    "Machine learning-based methods involve using machine learning algorithms to predict the missing values. These methods can be more accurate than other imputation methods if the machine learning algorithm is a good fit for the data. However, machine learning-based methods can be computationally expensive and require a large amount of data to train the algorithm.\n",
    "\n",
    "In conclusion, there are several methods for dealing with missing data in machine learning, each with its own advantages and disadvantages. The choice of method depends on the amount of missing data, the type of missingness, and the specific requirements of the analysis. Careful consideration of the implications of missing data and the selection of an appropriate imputation method are critical for ensuring accurate and reliable results.\n",
    "\n",
    "### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words.\n",
    "__Data Cleaning:__ The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It involves handling of missing data, noisy data etc.\n",
    "\n",
    "__Missing Data:__ This situation arises when some data is missing in the data. It can be handled in various ways. Some of them are:\n",
    "\n",
    "__Ignore the tuples:__ This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple.\n",
    "__Fill the Missing values:__ There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value.\n",
    "__Noisy Data:__ Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways :\n",
    "__Binning Method:__ This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete the task. Each segmented is handled separately. One can replace all data in a segment by its mean or boundary values can be used to complete the task.\n",
    "__Regression:__ Here data can be made smooth by fitting it to a regression function.The regression used may be linear (having one independent variable) or multiple (having multiple independent variables).\n",
    "__Clustering:__ This approach groups the similar data in a cluster. The outliers may be undetected or it will fall outside the clusters.\n",
    "__Data Reduction:__ Since data mining is a technique that is used to handle huge amount of data. While working with huge volume of data, analysis became harder in such cases. In order to get rid of this, we uses data reduction technique. It aims to increase the storage efficiency and reduce data storage and analysis costs. \n",
    "\n",
    "The various steps to data reduction are:\n",
    "\n",
    "__Data Cube Aggregation:__ Aggregation operation is applied to data for the construction of the data cube.\n",
    "__Attribute Subset Selection:__ The highly relevant attributes should be used, rest all can be discarded. For performing attribute selection, one can use level of significance and p- value of the attribute.the attribute having p-value greater than significance level can be discarded.\n",
    "__Numerosity Reduction:__ This enable to store the model of data instead of whole data, for example: Regression Models.\n",
    "__Dimensionality Reduction:__ This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:\n",
    "Wavelet transforms and PCA (Principal Component Analysis).\n",
    "__Feature selection__ is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension.\n",
    "\n",
    "### 9.  i. What is the IQR? What criteria are used to assess it?\n",
    "IQR stands for Interquartile Range, which is a measure of variability or dispersion in a dataset. It is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) of the dataset, and represents the range of the middle 50% of the data.\n",
    "\n",
    "The IQR is a useful measure of variability because it is less sensitive to outliers than other measures of dispersion, such as the range or standard deviation. This is because the IQR only considers the middle 50% of the data, which is less affected by extreme values than the full range of the data.\n",
    "\n",
    "The IQR can be used to assess the distribution of a dataset and identify potential outliers. One common criterion for identifying outliers is to define them as any data points that are more than 1.5 times the IQR below Q1 or above Q3. These data points are considered to be outside the \"whiskers\" of the boxplot, which is a graphical representation of the distribution of the data based on the IQR.\n",
    "\n",
    "Another criterion for assessing the IQR is to compare it to the range of the data. If the IQR is small relative to the range, it suggests that the data is highly variable, while if the IQR is large relative to the range, it suggests that the data is relatively uniform.\n",
    "\n",
    "In summary, the IQR is a measure of variability in a dataset that represents the range of the middle 50% of the data. It can be used to identify potential outliers and assess the distribution of the data, and is less sensitive to outliers than other measures of dispersion. The criteria used to assess the IQR include comparing it to the range of the data and using it to identify potential outliers.\n",
    "\n",
    "### ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?\n",
    "A box plot is a graphical representation of the distribution of a dataset that provides information about its central tendency, dispersion, and potential outliers. The following are the various components of a box plot:\n",
    "\n",
    "__Median (Q2):__ The median is a measure of central tendency that divides the dataset into two halves, with 50% of the data points below it and 50% above it. It is represented by a horizontal line inside the box.\n",
    "\n",
    "__Quartiles (Q1 and Q3):__ Quartiles are measures of central tendency that divide the dataset into four quarters, with 25% of the data points below Q1, 50% between Q1 and Q2, and 75% between Q2 and Q3. Q1 and Q3 are represented by the lower and upper boundaries of the box, respectively.\n",
    "\n",
    "__Interquartile Range (IQR):__ The IQR is the range between Q1 and Q3, representing the middle 50% of the data. It is represented by the height of the box.\n",
    "\n",
    "__Whiskers:__ The whiskers are vertical lines extending from the top and bottom of the box that represent the range of the data within 1.5 times the IQR from Q1 and Q3. Any data points beyond the whiskers are considered outliers.\n",
    "\n",
    "__Outliers:__ Outliers are data points that fall outside the whiskers and are represented by individual points or asterisks.\n",
    "\n",
    "The length of the lower whisker will surpass the upper whisker in length when the lower quartile (Q1) is closer to the minimum value of the data than the upper quartile (Q3) is to the maximum value of the data. This indicates that the data is more skewed towards lower values.\n",
    "\n",
    "Box plots can be used to identify outliers by looking for data points that fall outside the whiskers. Any data points beyond the whiskers are considered outliers, and can be flagged for further investigation or removed from the dataset, depending on the specific analysis being performed. Outliers can provide valuable information about the data and potential issues with the measurement or data collection process, but can also have a significant impact on the results of the analysis if left unaddressed.\n",
    "\n",
    "In summary, box plots are a useful tool for visualizing the distribution of a dataset and identifying potential outliers. The various components of a box plot include the median, quartiles, interquartile range, whiskers, and outliers, which provide information about the central tendency, dispersion, and potential outliers in the data.\n",
    "\n",
    "### 10. Make brief notes on any two of the following:\n",
    "#### 1. Data collected at regular intervals\n",
    "Data collected at regular intervals:\n",
    "\n",
    "Interval data is one of the two types of discrete data.\n",
    "An example of interval data is the data collected on a thermometer—its gradation or markings are equidistant.\n",
    "Unlike ordinal data, interval data always take numerical values where the distance between two points on the scale is standardised and equal.\n",
    "#### 2. The gap between the quartiles\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "\n",
    "### 11. Make a comparison between : \n",
    "\n",
    "#### The average and median: \n",
    "The mean (informally, the “average“) is found by adding all of the numbers together and dividing by the number of items in the set: 10 + 10 + 20 + 40 + 70 / 5 = 30. The median is found by ordering the set from lowest to highest and finding the exact middle. The median is just the middle number: 20\n",
    "\n",
    "#### Histogram and barplot:\n",
    "Histograms and box plots are very similar in that they both help to visualize and describe numeric data. Although histograms are better in determining the underlying distribution of the data, box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
