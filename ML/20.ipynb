{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da9618f",
   "metadata": {},
   "source": [
    "## ML 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fe25a",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines?\n",
    "2. What is the concept of a support vector?\n",
    "3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "6. Let's say you have used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "10. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "### Answer\n",
    "1. The underlying concept of Support Vector Machines (SVMs) is to find the optimal hyperplane that separates two classes in a high-dimensional feature space. The SVM algorithm tries to maximize the margin between the hyperplane and the closest points of each class, which are known as support vectors. SVMs can be used for both classification and regression tasks.\n",
    "\n",
    "2. A support vector is a data point that is closest to the decision boundary, i.e., the hyperplane that separates the two classes. The support vectors are the critical elements of an SVM classifier because they determine the position and orientation of the decision boundary.\n",
    "\n",
    "3. It is necessary to scale the inputs when using SVMs because SVMs are sensitive to the scale of the input features. Features with a larger scale can dominate the optimization process and make the algorithm biased towards those features. Therefore, it is essential to scale the input features to a similar range, such as between 0 and 1 or -1 and 1.\n",
    "\n",
    "4. Yes, an SVM classifier can output a confidence score or a probability estimate. The confidence score represents the distance between the test instance and the decision boundary. However, SVMs do not provide a direct probability estimate, so the confidence score needs to be transformed into a probability estimate using a sigmoid or a softmax function.\n",
    "\n",
    "5. When training a model on a large dataset with many features, it is recommended to use the dual form of the SVM problem because it is computationally more efficient than the primal form. The dual form involves solving a smaller set of linear equations, while the primal form requires the optimization of a quadratic objective function.\n",
    "\n",
    "6. If an SVM classifier trained with an RBF kernel appears to underfit the training set, it is better to raise the gamma parameter, which controls the width of the kernel function. Increasing gamma makes the kernel more peaked, which can result in a more complex decision boundary. Similarly, increasing the C parameter, which controls the trade-off between margin size and classification errors, can also result in a more complex decision boundary.\n",
    "\n",
    "7. The QP parameters (H, f, A, and b) for solving the soft margin linear SVM classifier problem should be set as follows: H is a matrix that represents the quadratic terms of the objective function and can be computed from the training data; f is a vector that represents the linear terms of the objective function and can be computed from the training data; A is a matrix that represents the constraints on the variables and is usually set to the identity matrix or a matrix with -1 and 1 entries; and b is a vector that represents the bounds on the constraints and is set to 0 or 1 depending on the type of constraint.\n",
    "\n",
    "8. To train a LinearSVC, SVC, and SGDClassifier on a linearly separable dataset, you can use the same dataset and the same hyperparameters for each model. The resulting models should be similar in terms of accuracy and performance. However, the SGDClassifier may converge faster than the other models due to its stochastic optimization algorithm.\n",
    "\n",
    "9. To train an SVM classifier on the MNIST dataset, you can use a one-versus-the-rest strategy to classify all ten digits. To accelerate the process, you can use a small validation set to tune the hyperparameters, such as the C parameter and the gamma parameter. The level of precision that can be achieved depends on the quality of the data preprocessing, feature engineering, and hyperparameter tuning.\n",
    "\n",
    "10. To train an SVM regressor on the California housing dataset, you can use the same approach as for classification, but instead of finding a hyperplane that separates two classes, you are finding a hyperplane that best fits the data. SVM regression involves minimizing the distance between the hyperplane and the training instances while penalizing instances that are outside the margin. The The performance of the SVM regressor on the California housing dataset depends on the quality of the input features, the choice of kernel function, and the hyperparameter tuning. It is important to preprocess the data, handle missing values, and normalize the input features before training the SVM regressor. Additionally, it is necessary to select the appropriate kernel function, such as linear, polynomial, or RBF, based on the nature of the data and the complexity of the problem. Finally, hyperparameter tuning can be performed using a validation set or a grid search to find the optimal values for parameters such as C, gamma, and epsilon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
