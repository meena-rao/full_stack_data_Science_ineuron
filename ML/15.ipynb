{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0eb1166",
   "metadata": {},
   "source": [
    "## ML15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a6bcf",
   "metadata": {},
   "source": [
    "1. Recognize the differences between supervised, semi-supervised, and unsupervised learning.\n",
    "2. Describe in detail any five examples of classification problems.\n",
    "3. Describe each phase of the classification process in detail.\n",
    "\n",
    "4. Go through the SVM model in depth using various scenarios.\n",
    "\n",
    "5. What are some of the benefits and drawbacks of SVM?\n",
    "\n",
    "6. Go over the kNN model in depth.\n",
    "\n",
    "7. Discuss the kNN algorithm&#39;s error rate and validation error.\n",
    "\n",
    "8. For kNN, talk about how to measure the difference between the test and training results.\n",
    "\n",
    "9. Create the kNN algorithm.\n",
    "\n",
    "10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth.\n",
    "\n",
    "11. Describe the different ways to scan a decision tree.\n",
    "\n",
    "12. Describe in depth the decision tree algorithm.\n",
    "\n",
    "13. In a decision tree, what is inductive bias? What would you do to stop overfitting?\n",
    "\n",
    "14.Explain advantages and disadvantages of using a decision tree?\n",
    "\n",
    "15. Describe in depth the problems that are suitable for decision tree learning.\n",
    "\n",
    "16. Describe in depth the random forest model. What distinguishes a random forest?\n",
    "\n",
    "17. In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6c47e",
   "metadata": {},
   "source": [
    "1. Supervised learning involves a labeled dataset where the machine learning model learns from the input-output pairs to predict outcomes for new inputs. Semi-supervised learning uses both labeled and unlabeled data to train the model, where the labeled data is used to guide the learning from unlabeled data. Unsupervised learning involves an unlabeled dataset where the machine learning model tries to find patterns, structures, or relationships within the data.\n",
    "\n",
    "2. Examples of classification problems include predicting whether an email is spam or not, classifying images into different categories such as cats and dogs, detecting fraudulent transactions, predicting whether a loan applicant will default or not, and predicting whether a patient will develop a certain disease or not.\n",
    "\n",
    "3. The classification process involves several phases, including data preprocessing, feature extraction, model training, model selection, and model evaluation. Data preprocessing involves cleaning and transforming the data to make it suitable for analysis. Feature extraction involves selecting the most relevant features to train the model. Model training involves selecting a suitable algorithm and training the model on the data. Model selection involves evaluating different models and selecting the best one based on their performance. Model evaluation involves testing the selected model on new data to assess its performance.\n",
    "\n",
    "4. The SVM model is a supervised learning algorithm used for classification and regression analysis. It works by finding the hyperplane that separates the data into different classes. The SVM model can handle both linear and nonlinear data using different kernels such as the linear, polynomial, and radial basis function (RBF) kernels. The SVM model also has a regularization parameter that controls the trade-off between model complexity and classification error.\n",
    "\n",
    "5. The benefits of SVM include its ability to handle high-dimensional data, its effectiveness in handling both linear and nonlinear data, and its ability to handle noise and outliers. The drawbacks of SVM include its sensitivity to the choice of kernel function and hyperparameters, its complexity in handling large datasets, and its lack of transparency in model interpretation.\n",
    "\n",
    "6. The kNN model is a non-parametric supervised learning algorithm used for classification and regression analysis. It works by finding the k-nearest neighbors to the input data point and assigning the class label based on the majority vote of its neighbors. The kNN model is sensitive to the choice of k and the distance metric used to measure the similarity between data points.\n",
    "\n",
    "7. The kNN algorithm's error rate is the proportion of misclassified instances in the test set. The validation error is the error rate obtained by testing the model on a validation set, which is used to select the best value of k and distance metric.\n",
    "\n",
    "8. The difference between the test and training results can be measured using the error rate or accuracy metric. The test results are obtained by testing the model on a separate test set that was not used during the training phase. The training results are obtained by testing the model on the same data used for training.\n",
    "\n",
    "9. Here is an example of the kNN algorithm:\n",
    "\n",
    "Select the value of k and distance metric\n",
    "Calculate the distance between the input data point and all the training data points\n",
    "Select the k-nearest neighbors based on the distance metric\n",
    "Assign the class label based on the majority vote of the neighbors\n",
    "Repeat for each input data point\n",
    "\n",
    "10. A decision tree is a supervised learning algorithm used for classification and regression analysis. It works by partitioning the data into subsets based on the values of the input features and assigning class labels to each subset. The various kinds of nodes in a decision tree include the root node, internal nodes, leaf nodes, and branches. The root node represents the entire dataset, the internal nodes represent the partitioning of the data based on the input features, the leaf nodes represent the final classification or regression output, and the branches represent the decision rules based on the input features.\n",
    "\n",
    "11. The different ways toscan a decision tree include pre-order traversal, in-order traversal, and post-order traversal. Pre-order traversal involves visiting the root node first, followed by its left and right subtrees recursively. In-order traversal involves visiting the left subtree first, followed by the root node, and then the right subtree recursively. Post-order traversal involves visiting the left and right subtrees first recursively, followed by the root node.\n",
    "\n",
    "12. The decision tree algorithm works by selecting the best input feature to partition the data based on a certain criterion such as information gain or Gini index. It then creates a node for the selected feature and partitions the data based on its values. It repeats this process recursively for each subset of the data until a stopping criterion is met. The stopping criterion can be a maximum depth, minimum number of instances in a leaf node, or a minimum decrease in impurity.\n",
    "\n",
    "13. Inductive bias in a decision tree refers to the assumptions and biases that the algorithm makes based on the training data. To prevent overfitting, one can use techniques such as pruning, regularization, or ensemble methods. Pruning involves removing branches or nodes from the tree to reduce its complexity. Regularization involves adding a penalty term to the objective function to control the model's complexity. Ensemble methods involve combining multiple decision trees to improve the model's performance and reduce overfitting.\n",
    "\n",
    "14. The advantages of using a decision tree include its simplicity, interpretability, and ability to handle both categorical and continuous data. The disadvantages of using a decision tree include its tendency to overfit the data, its sensitivity to small changes in the data, and its lack of robustness to noise and outliers.\n",
    "\n",
    "15. Decision tree learning is suitable for problems where the input features have discrete or continuous values and the output variable is categorical or continuous. It is particularly useful for problems where interpretability is important and the decision-making process is based on a set of rules.\n",
    "\n",
    "16. The random forest model is an ensemble learning algorithm that combines multiple decision trees to improve the model's performance and reduce overfitting. It works by creating a set of decision trees using random subsets of the data and input features. The final prediction is obtained by averaging the predictions of all the trees. What distinguishes a random forest is its use of random subsampling and feature selection to reduce the correlation between the trees and increase their diversity.\n",
    "\n",
    "17. In a random forest, the OOB error is the error rate obtained by testing the model on the data points that were not used during the training of each tree. Variable importance measures the importance of each input feature in the model by measuring how much the prediction error increases when that feature is randomly permuted. It can be used to identify the most important features in the data and to perform feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
