{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2b5f2f",
   "metadata": {},
   "source": [
    "## ML16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12439b",
   "metadata": {},
   "source": [
    "1. In a linear equation, what is the difference between a dependent variable and an independent\n",
    "variable?\n",
    "2. What is the concept of simple linear regression? Give a specific example.\n",
    "3. In a linear regression, define the slope.\n",
    "\n",
    "4. Determine the graph&#39;s slope, where the lower point on the line is represented as (3, 2) and the\n",
    "higher point is represented as (2, 2).\n",
    "\n",
    "5. In linear regression, what are the conditions for a positive slope?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706d641",
   "metadata": {},
   "source": [
    "### Answer from 1 to 5\n",
    "1. In a linear equation, the independent variable is the variable that is being manipulated or controlled in order to observe its effect on the dependent variable, which is the variable that is being measured or observed. The dependent variable depends on the value of the independent variable.\n",
    "\n",
    "2. Simple linear regression is a statistical method used to model the relationship between two variables by fitting a linear equation to the observed data. The goal of simple linear regression is to find the line of best fit that describes the relationship between the variables. For example, a researcher may use simple linear regression to study the relationship between a person's age and their height.\n",
    "\n",
    "3. In a linear regression, the slope is the coefficient that represents the change in the dependent variable for a one-unit change in the independent variable. It describes the steepness of the line of best fit. A positive slope indicates that the dependent variable increases as the independent variable increases, while a negative slope indicates that the dependent variable decreases as the independent variable increases.\n",
    "\n",
    "4. The slope of the line can be calculated using the formula:\n",
    "\n",
    "slope = (change in y) / (change in x)\n",
    "\n",
    "In this case, the change in x is -1 (2-3) and the change in y is 0 (2-2). Therefore, the slope is:\n",
    "\n",
    "slope = 0 / (-1) = 0\n",
    "\n",
    "So the graph has a slope of 0, which means that the line is horizontal.\n",
    "\n",
    "5. In linear regression, a positive slope indicates that as the independent variable increases, the dependent variable also increases. The conditions for a positive slope are:\n",
    "The correlation between the two variables is positive (i.e., as one variable increases, the other variable tends to increase as well).\n",
    "The residuals (the differences between the actual values and the predicted values) are small and normally distributed.\n",
    "There are no outliers or influential observations that could affect the slope of the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670679f4",
   "metadata": {},
   "source": [
    "### Anser from 6 to 10\n",
    "6. In linear regression, what are the conditions for a negative slope?\n",
    "\n",
    "7. What is multiple linear regression and how does it work?\n",
    "\n",
    "8. In multiple linear regression, define the number of squares due to error.\n",
    "\n",
    "9. In multiple linear regression, define the number of squares due to regression.\n",
    "\n",
    "10. In a regression equation, what is multicollinearity?\n",
    "\n",
    "\n",
    "6. In linear regression, a negative slope indicates that as the independent variable increases, the dependent variable decreases. The conditions for a negative slope are:\n",
    "The correlation between the two variables is negative (i.e., as one variable increases, the other variable tends to decrease).\n",
    "The residuals are small and normally distributed.\n",
    "There are no outliers or influential observations that could affect the slope of the line of best fit.\n",
    "\n",
    "7. Multiple linear regression is a statistical method used to model the relationship between a dependent variable and multiple independent variables by fitting a linear equation to the observed data. It extends simple linear regression by allowing for more than one predictor variable. The goal of multiple linear regression is to find the line of best fit that describes the relationship between the variables. It works by minimizing the sum of the squared errors between the observed values and the predicted values.\n",
    "\n",
    "8. In multiple linear regression, the number of squares due to error (SSE) is a measure of the variability in the dependent variable that cannot be explained by the model. It represents the sum of the squared differences between the observed values and the predicted values, weighted by the degrees of freedom.\n",
    "\n",
    "9. In multiple linear regression, the number of squares due to regression (SSR) is a measure of the variability in the dependent variable that is explained by the model. It represents the sum of the squared differences between the predicted values and the mean of the dependent variable, weighted by the degrees of freedom.\n",
    "\n",
    "10. In a regression equation, multicollinearity refers to the presence of high correlation between two or more independent variables. It can be a problem in regression analysis because it can make it difficult to determine the individual effect of each independent variable on the dependent variable. Multicollinearity can lead to unstable and unreliable estimates of the regression coefficients, and it can make it difficult to interpret the results of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d464a",
   "metadata": {},
   "source": [
    "### Answer from 11 to 16\n",
    "\n",
    "11. What is heteroskedasticity, and what does it mean?\n",
    "\n",
    "12. Describe the concept of ridge regression.\n",
    "\n",
    "13. Describe the concept of lasso regression.\n",
    "\n",
    "14. What is polynomial regression and how does it work?\n",
    "\n",
    "15. Describe the basis function.\n",
    "\n",
    "16. Describe how logistic regression works.\n",
    "\n",
    "11 Heteroskedasticity refers to a situation where the variance of the errors in a regression model is not constant across all levels of the independent variables. In other words, the spread of the residuals changes as the value of the independent variable changes. Heteroskedasticity can lead to biased and inefficient estimates of the regression coefficients and can make it difficult to determine the significance of the independent variables.\n",
    "\n",
    "12 Ridge regression is a technique used to address multicollinearity in regression analysis. It works by adding a penalty term to the sum of the squared errors in the regression equation, which helps to shrink the regression coefficients towards zero. The penalty term is based on the sum of the squares of the regression coefficients, and it is controlled by a tuning parameter called the regularization parameter.\n",
    "\n",
    "13 Lasso regression is another technique used to address multicollinearity in regression analysis. It works by adding a penalty term to the sum of the absolute values of the regression coefficients in the regression equation. Like ridge regression, lasso regression helps to shrink the regression coefficients towards zero, but it has the additional advantage of performing variable selection by setting some of the coefficients to exactly zero.\n",
    "\n",
    "14 Polynomial regression is a technique used to model the relationship between a dependent variable and an independent variable by fitting a polynomial function to the observed data. It works by adding additional terms to the regression equation that include higher powers of the independent variable. For example, a quadratic regression model includes a squared term of the independent variable.\n",
    "\n",
    "15 Basis functions are a set of functions used in regression analysis to represent the relationship between the independent variable and the dependent variable. They are typically used in non-linear regression models and can be any function that transforms the original input data into a new feature space. Examples of basis functions include polynomials, radial basis functions, and Fourier basis functions.\n",
    "\n",
    "16 Logistic regression is a statistical method used to model the probability of a binary outcome (i.e., a dichotomous dependent variable) based on one or more independent variables. It works by fitting a logistic function to the observed data, which maps the independent variables onto a continuous scale between 0 and 1. The logistic function is used to estimate the probability of the dependent variable being in one category (e.g., success) versus the other category (e.g., failure) as a function of the independent variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
