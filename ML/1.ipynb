{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be5ba176",
   "metadata": {},
   "source": [
    "### ML 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54003b",
   "metadata": {},
   "source": [
    "#### 1. What does one mean by the term machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57c90b",
   "metadata": {},
   "source": [
    "\"ML\" typically stands for \"machine learning,\" which is a type of artificial intelligence that involves training algorithms to automatically improve their performance on a specific task by learning from data. In machine learning, a computer program is not explicitly programmed to perform a task; instead, it is trained on a large dataset to learn patterns and make predictions or decisions based on that learning. Machine learning has many applications, including natural language processing, computer vision, predictive modeling, and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489cf560",
   "metadata": {},
   "source": [
    "#### 2.Can you think of 4 distinct types of issues where it shines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceebed4",
   "metadata": {},
   "source": [
    "Pattern recognition: Machine learning can excel in tasks where there are complex patterns to be identified in data. For example, image recognition tasks, where a machine learning algorithm can learn to identify objects, people, or animals in images with high accuracy.\n",
    "\n",
    "Predictive modeling: Machine learning can be used to make accurate predictions based on historical data. For instance, machine learning algorithms can be trained to predict stock prices, customer behavior, or medical diagnosis.\n",
    "\n",
    "Natural Language Processing: Machine learning can be used to understand and generate natural language. It can be applied in tasks such as sentiment analysis, chatbots, or speech recognition.\n",
    "\n",
    "Anomaly detection: Machine learning can be used to identify unusual events or outliers in data. For example, machine learning algorithms can be trained to detect fraudulent transactions, network intrusions, or equipment failure in manufacturing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8f53a",
   "metadata": {},
   "source": [
    "##### 3.What is a labeled training set, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c0f5e",
   "metadata": {},
   "source": [
    "A labeled training set is a dataset used in supervised machine learning, where each data point is associated with a label or target variable. The label indicates the correct output for the corresponding input data point.\n",
    "\n",
    "For example, in a binary classification problem to identify whether an email is spam or not, the training set would consist of a collection of emails and their corresponding labels of \"spam\" or \"not spam\". The labeled training set is used to train a machine learning algorithm to recognize the patterns in the input data and make accurate predictions on new, unseen data.\n",
    "\n",
    "During the training process, the machine learning algorithm learns to map the input data to the correct output by adjusting its internal parameters. The algorithm iteratively makes predictions on the labeled training data and compares them to the correct labels. It then updates its internal parameters to minimize the difference between the predicted output and the correct label. This process is repeated until the algorithm's performance on the training set reaches a satisfactory level.\n",
    "\n",
    "Once the machine learning algorithm is trained on the labeled training set, it can be used to make predictions on new, unseen data with similar input features. The accuracy of the predictions depends on the quality of the labeled training set, the complexity of the algorithm, and the similarity of the new data to the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1a19d",
   "metadata": {},
   "source": [
    "#### 4.What are the two most important tasks that are supervised?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e0be1",
   "metadata": {},
   "source": [
    "In supervised machine learning, there are two important tasks that are commonly used:\n",
    "\n",
    "Regression: Regression is a task where the goal is to predict a continuous numerical output value based on input features. In regression, the machine learning algorithm learns a function that maps the input features to the output variable. The goal is to minimize the difference between the predicted output and the true output for a given set of input features.\n",
    "\n",
    "Classification: Classification is a task where the goal is to predict a categorical output value based on input features. In classification, the machine learning algorithm learns a decision boundary that separates the input features into different categories. The goal is to minimize the number of misclassifications on a given set of labeled data.\n",
    "\n",
    "Both regression and classification are supervised learning tasks because they require labeled training data, where the correct output values are known for each input feature set. These tasks have many practical applications, such as predicting housing prices, diagnosing medical conditions, and detecting spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131e4de",
   "metadata": {},
   "source": [
    "#### 5.Can you think of four examples of unsupervised tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcea793",
   "metadata": {},
   "source": [
    "Clustering: Clustering is an unsupervised learning task where the goal is to group similar data points together in a dataset without any prior knowledge of the groups. The algorithm tries to find patterns in the data and group similar data points into clusters based on their features.\n",
    "\n",
    "Dimensionality reduction: Dimensionality reduction is an unsupervised task where the goal is to reduce the number of features in a dataset while preserving the most important information. The algorithm tries to find a lower-dimensional representation of the data that captures the most important patterns in the original data.\n",
    "\n",
    "Anomaly detection: Anomaly detection is an unsupervised task where the goal is to identify rare or unusual data points in a dataset that differ significantly from the majority of the data points. The algorithm tries to find patterns in the data and identify data points that deviate significantly from those patterns.\n",
    "\n",
    "Association rule mining: Association rule mining is an unsupervised task where the goal is to discover relationships between variables in a dataset. The algorithm tries to find patterns in the data and identify co-occurring variables or items in the data that tend to appear together. This task is commonly used in market basket analysis to identify which products are often bought together by customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f330089",
   "metadata": {},
   "source": [
    "#### 6.State the machine learning model that would be best to make a robot walk through various unfamiliar terrains?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475a5c2",
   "metadata": {},
   "source": [
    "To make a robot walk through various unfamiliar terrains, the most suitable machine learning model would be a reinforcement learning (RL) algorithm.\n",
    "\n",
    "Reinforcement learning is a type of machine learning that involves training an agent to make sequential decisions in an environment to maximize a cumulative reward signal. In this case, the agent is the robot, the environment is the various unfamiliar terrains, and the reward signal could be the robot's ability to move forward or avoid obstacles.\n",
    "\n",
    "The reinforcement learning algorithm would work by allowing the robot to interact with the environment and learn from its experiences. The robot would receive feedback from the environment in the form of rewards or punishments based on its actions. Over time, the robot would learn which actions lead to higher rewards and which ones lead to lower rewards. The reinforcement learning algorithm would adjust the robot's behavior to maximize the cumulative reward signal.\n",
    "\n",
    "To implement reinforcement learning for the robot walking task, the algorithm would need to define the state space, action space, and reward function for the robot. The state space would define the robot's current position and orientation, while the action space would define the possible movements that the robot could make. The reward function would define the goal of the robot's task, such as moving forward or avoiding obstacles.\n",
    "\n",
    "By using reinforcement learning, the robot would be able to adapt to different terrains and learn from its experiences, allowing it to make decisions and navigate through new environments with greater accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb6f6c",
   "metadata": {},
   "source": [
    "#### 7.Which algorithm will you use to divide your customers into different groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91786789",
   "metadata": {},
   "source": [
    "To divide customers into different groups, a clustering algorithm would be the most suitable choice. Clustering is an unsupervised machine learning task that aims to group similar data points together based on their features.\n",
    "\n",
    "There are many different clustering algorithms available, but some of the most commonly used ones are K-means, Hierarchical clustering, and Density-based clustering. The choice of algorithm depends on the specific requirements of the problem and the nature of the data.\n",
    "\n",
    "In the case of customer segmentation, the first step would be to identify the relevant features or variables that define the customers' characteristics, such as age, gender, location, purchase history, etc. Once the features have been selected, the clustering algorithm would group similar customers together based on the values of these features.\n",
    "\n",
    "For example, K-means clustering could be used to group customers into K clusters, where K is a predefined number. The algorithm would randomly assign the customers to different clusters and then iteratively refine the clusters until the assignments converge to a stable configuration. Each cluster would represent a distinct group of customers with similar characteristics.\n",
    "\n",
    "Alternatively, Hierarchical clustering could be used to create a dendrogram that represents the relationship between the clusters in a tree-like structure. This method does not require a predefined number of clusters, as the dendrogram can be cut at different levels to create different numbers of clusters.\n",
    "\n",
    "Density-based clustering methods, such as DBSCAN, could also be used to identify dense regions of the data and group customers together based on their proximity to these regions.\n",
    "\n",
    "Overall, the choice of clustering algorithm depends on the nature of the data and the specific requirements of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532d276",
   "metadata": {},
   "source": [
    "#### 8.Will you consider the problem of spam detection to be a supervised or unsupervised learning\n",
    "problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73af41b",
   "metadata": {},
   "source": [
    "The problem of spam detection is generally considered to be a supervised learning problem.\n",
    "\n",
    "Supervised learning involves training a model on a labeled dataset, where the desired output or target variable is known for each data point. In the case of spam detection, the target variable would be a binary label indicating whether an email is spam or not.\n",
    "\n",
    "To train a supervised learning model for spam detection, a dataset of labeled emails would be needed, where each email is labeled as spam or not spam. The model would learn to recognize patterns in the input data (i.e., the text of the email) that are associated with spam emails and use these patterns to make predictions on new, unseen emails.\n",
    "\n",
    "On the other hand, unsupervised learning is used when the target variable is unknown and the goal is to discover hidden patterns or structures in the data. This type of learning is often used in clustering or anomaly detection tasks, where the algorithm groups similar data points together or identifies unusual patterns in the data.\n",
    "\n",
    "While unsupervised learning methods could potentially be used for spam detection (e.g., by identifying unusual patterns in the text of an email that are not typical of legitimate emails), they are less commonly used for this purpose than supervised learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a196a1",
   "metadata": {},
   "source": [
    "#### 9.What is the concept of an online learning system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec228764",
   "metadata": {},
   "source": [
    "An online learning system is a machine learning system that is designed to continuously learn from new data as it becomes available, rather than being trained on a fixed dataset. This is sometimes referred to as incremental learning or online learning.\n",
    "\n",
    "In an online learning system, the model is updated in real-time as new data is received, rather than being retrained from scratch on the entire dataset. This allows the model to adapt to changes in the data over time and incorporate new information into its predictions.\n",
    "\n",
    "Online learning is often used in scenarios where the data is constantly changing or where the cost of retraining the model on the entire dataset is prohibitive. For example, online learning could be used in fraud detection systems to continuously adapt to new types of fraudulent behavior or in recommender systems to incorporate users' recent interactions with the system.\n",
    "\n",
    "Online learning systems typically use algorithms that are designed to update the model efficiently and incrementally. Examples of online learning algorithms include stochastic gradient descent and online versions of clustering algorithms such as Online K-Means.\n",
    "\n",
    "While online learning has some advantages over traditional batch learning methods, it also presents some unique challenges, such as handling concept drift and ensuring the model does not overfit to the most recent data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b2d42",
   "metadata": {},
   "source": [
    "#### 10.What is out-of-core learning, and how does it differ from core learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e2bbe",
   "metadata": {},
   "source": [
    "Out-of-core learning is a technique used to train machine learning models on very large datasets that do not fit entirely into memory. It involves breaking up the dataset into smaller, manageable chunks and training the model on each chunk separately, updating the model parameters as new data becomes available.\n",
    "\n",
    "The term \"out-of-core\" refers to the fact that the data is stored on disk rather than in memory, allowing the model to process the data in chunks as needed. This is in contrast to \"in-core\" or \"in-memory\" learning, where the entire dataset is loaded into memory at once.\n",
    "\n",
    "Out-of-core learning is typically used when the size of the dataset exceeds the available memory on a single machine or when the cost of storing the entire dataset in memory is prohibitive. It is commonly used in scenarios such as training models on large text or image datasets.\n",
    "\n",
    "The main difference between out-of-core learning and in-core learning is in the way the data is processed. In-core learning algorithms assume that the entire dataset is available in memory and can be accessed quickly, which allows for more efficient computations. Out-of-core learning algorithms, on the other hand, must be designed to work with data that is stored on disk and may require more time to access and process each chunk of data.\n",
    "\n",
    "Out-of-core learning typically requires a more complex implementation than in-core learning, as the algorithm must manage the data in a way that allows for efficient processing of each chunk while maintaining the model's accuracy over time. Examples of out-of-core learning algorithms include Stochastic Gradient Descent (SGD) and Mini-Batch K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78464c",
   "metadata": {},
   "source": [
    "#### 11.What kind of learning algorithm makes predictions using a similarity measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a01e76",
   "metadata": {},
   "source": [
    "A learning algorithm that makes predictions using a similarity measure is typically a type of unsupervised learning algorithm known as a clustering algorithm.\n",
    "\n",
    "Clustering algorithms group similar data points together based on some measure of similarity, such as distance or correlation. Once the data points have been grouped into clusters, the algorithm can make predictions based on the characteristics of each cluster.\n",
    "\n",
    "For example, one common similarity measure used in clustering algorithms is Euclidean distance, which measures the straight-line distance between two points in a multi-dimensional space. In this case, the algorithm would group together data points that are close to each other in the multi-dimensional space and make predictions based on the characteristics of each group.\n",
    "\n",
    "Another example of a similarity measure used in clustering algorithms is cosine similarity, which measures the cosine of the angle between two vectors in a high-dimensional space. This measure is often used in natural language processing tasks, where the algorithm is trying to group together similar documents or words based on their content.\n",
    "\n",
    "Overall, clustering algorithms that use similarity measures are useful for discovering patterns and structures in large datasets without the need for labeled data. However, they typically require some degree of human interpretation to make sense of the resulting clusters and to use them for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad95d5",
   "metadata": {},
   "source": [
    "#### 12.What isthe difference between a model parameter and a hyperparameter in a learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e1917",
   "metadata": {},
   "source": [
    "In a machine learning algorithm, a model parameter is a variable that is learned during the training process and is used to make predictions on new data. The value of a model parameter is determined by the learning algorithm, which adjusts the parameter to minimize the difference between the predicted output and the actual output on the training data. Examples of model parameters include the weights in a neural network or the coefficients in a linear regression model.\n",
    "\n",
    "A hyperparameter, on the other hand, is a setting or configuration option that is chosen by the user before the learning process begins. Hyperparameters are not learned by the algorithm but rather set by the user to control the learning process. Examples of hyperparameters include the learning rate in gradient descent or the number of hidden layers in a neural network.\n",
    "\n",
    "The key difference between model parameters and hyperparameters is that model parameters are learned by the algorithm during training, while hyperparameters are set by the user before training begins. Model parameters are used to make predictions on new data, while hyperparameters are used to control the learning process and determine the optimal values of the model parameters.\n",
    "\n",
    "Choosing the right hyperparameters is important in machine learning, as the performance of the model can depend on their values. Hyperparameter tuning is the process of selecting the best values for the hyperparameters of a model, often using techniques such as grid search or random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b2a73",
   "metadata": {},
   "source": [
    "#### 13.What are the criteria that model-based learning algorithms look for? What is the most popular method they use to achieve success? What method do they use to make predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa4908",
   "metadata": {},
   "source": [
    "Model-based learning algorithms look for a model that can accurately predict the output for new, unseen data based on the input data. The criteria they typically use to evaluate the accuracy of a model include the error rate or accuracy on the training data, the error rate or accuracy on the validation data, and the error rate or accuracy on the test data.\n",
    "\n",
    "To achieve success, model-based learning algorithms often use techniques such as regularization, cross-validation, and ensemble methods. Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function that encourages the model to have smaller weights or coefficients. Cross-validation is a technique used to evaluate the performance of the model on multiple subsets of the data and to choose the best hyperparameters. Ensemble methods involve combining multiple models together to improve the overall accuracy of the predictions.\n",
    "\n",
    "To make predictions, model-based learning algorithms use the learned model to map the input data to the output data. For example, in a linear regression model, the model-based learning algorithm would use the learned coefficients to predict the output value for a given input value. In a decision tree model, the model-based learning algorithm would use the learned tree structure to classify new input data into one of several possible categories.\n",
    "\n",
    "Overall, model-based learning algorithms are popular because they can be used to build accurate models for a wide range of applications, including classification, regression, and clustering. However, they can be computationally expensive and may require a large amount of training data to achieve good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce61f0d",
   "metadata": {},
   "source": [
    "#### 14.Can you name four of the most important Machine Learning challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24868883",
   "metadata": {},
   "source": [
    "Data quality and quantity: Machine learning models rely heavily on the quality and quantity of the training data. If the data is incomplete, inaccurate, or biased, the model's performance may suffer. Obtaining large amounts of high-quality labeled data can be a significant challenge in many applications.\n",
    "\n",
    "Overfitting and underfitting: Overfitting occurs when a model becomes too complex and fits the training data too closely, leading to poor performance on new, unseen data. Underfitting occurs when a model is too simple and fails to capture the underlying patterns in the data. Balancing the complexity of the model with its ability to generalize to new data is a key challenge in machine learning.\n",
    "\n",
    "Model selection and hyperparameter tuning: There are many different types of machine learning models, each with its own strengths and weaknesses. Choosing the right model and tuning its hyperparameters can be challenging and can require significant expertise.\n",
    "\n",
    "Interpretability and explainability: As machine learning models become more complex, it can become difficult to understand how they make predictions or decisions. This can be a particular challenge in applications where the consequences of a wrong prediction or decision could be severe, such as in healthcare or finance. Developing models that are interpretable and explainable is an important area of research in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6cfc57",
   "metadata": {},
   "source": [
    "#### 15.What happens if the model performs well on the training data but fails to generalize the results to new situations? Can you think of three different options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b023fc",
   "metadata": {},
   "source": [
    "If a model performs well on the training data but fails to generalize to new situations, it is said to have overfit to the training data. Overfitting occurs when the model learns the noise in the training data rather than the underlying patterns. Here are three different options to address overfitting:\n",
    "\n",
    "Regularization: Regularization is a technique that adds a penalty term to the cost function to discourage the model from overfitting the training data. The penalty term is typically a function of the model's weights or parameters, and it encourages the model to have smaller weights. Regularization can help prevent overfitting and improve the model's ability to generalize to new data.\n",
    "\n",
    "Cross-validation: Cross-validation is a technique that can help identify overfitting by evaluating the model's performance on multiple subsets of the data. By evaluating the model on multiple subsets of the data, it is possible to get a better estimate of how well the model will generalize to new data. If the model performs well on the training data but poorly on the validation data, it is likely overfitting the training data.\n",
    "\n",
    "Simplifying the model: If the model is too complex and overfits the training data, simplifying the model can help. This might involve reducing the number of features or inputs to the model, decreasing the number of layers in a neural network, or reducing the number of nodes in a decision tree. Simplifying the model can help prevent overfitting and improve the model's ability to generalize to new data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b400835",
   "metadata": {},
   "source": [
    "#### 16.What exactly is a test set, and why would you need one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96e6f5",
   "metadata": {},
   "source": [
    "A test set is a portion of the data that is held out from the model during training and used to evaluate the model's performance on new, unseen data. Typically, the original data set is split into two or three parts: a training set, a validation set (optional), and a test set. The model is trained on the training set and validated on the validation set (if present), and the test set is used to evaluate the final performance of the model.\n",
    "\n",
    "The reason why a test set is needed is to provide an unbiased estimate of the model's performance on new, unseen data. If the model is evaluated on the same data that was used for training, it may overestimate its performance and not generalize well to new data. The test set provides a way to evaluate the model's ability to generalize to new data that was not seen during training.\n",
    "\n",
    "Using a test set is an important practice in machine learning because it helps to prevent overfitting and ensures that the model is truly learning the underlying patterns in the data rather than just memorizing the training data. It also helps to provide a fair evaluation of the model's performance that can be used to compare different models and make decisions about which model to use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0176c",
   "metadata": {},
   "source": [
    "#### 17.What is a validation set& its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c5175",
   "metadata": {},
   "source": [
    "A validation set is a portion of the original data that is used to tune the hyperparameters of a machine learning model during the training process. In machine learning, hyperparameters are parameters that cannot be learned from the data and must be set by the user before training begins. Examples of hyperparameters include the learning rate, the number of hidden layers in a neural network, and the number of trees in a random forest.\n",
    "\n",
    "The purpose of a validation set is to provide a way to evaluate the performance of the model on new data that was not seen during training. During training, the model is evaluated on the validation set after each epoch (i.e., pass through the training data), and the hyperparameters are adjusted based on the validation set performance. The goal is to find the hyperparameters that result in the best validation set performance, which is typically a proxy for the model's performance on new, unseen data.\n",
    "\n",
    "Using a validation set is an important practice in machine learning because it helps to prevent overfitting and ensures that the model is able to generalize well to new data. By tuning the hyperparameters on a separate validation set, it is possible to get a more accurate estimate of the model's performance on new data and to improve the model's ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2321e",
   "metadata": {},
   "source": [
    "#### 18.What precisely is the train-dev kit, when will you need it, how do you put it to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea740f",
   "metadata": {},
   "source": [
    "The train-dev set, also known as the development set, is a subset of the training data that is used to monitor the model's performance during training and to diagnose training issues such as overfitting or underfitting.\n",
    "\n",
    "The purpose of the train-dev set is to provide a way to evaluate the model's performance on a set of data that is similar to the training data but is distinct from the validation and test data sets. This helps to detect and diagnose any problems with the training process, such as overfitting to the training data, which can occur when the model becomes too complex and starts to memorize the training data rather than learning the underlying patterns.\n",
    "\n",
    "To use the train-dev set, you would first split the original data set into three parts: the training set, the validation set, and the train-dev set. During training, the model is evaluated on the validation set, and the hyperparameters are tuned based on the validation set performance. Then, the model is evaluated on the train-dev set to monitor its performance on data that is similar to the training data but is distinct from the validation and test data sets.\n",
    "\n",
    "If the performance of the model on the train-dev set is significantly worse than its performance on the validation set, it indicates that the model is overfitting to the training data and may need to be simplified. If the performance of the model on the train-dev set is significantly better than its performance on the validation set, it indicates that the model is underfitting and may need to be made more complex. By monitoring the model's performance on the train-dev set, you can diagnose and fix any issues with the training process and ensure that the model is able to generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b335e",
   "metadata": {},
   "source": [
    "#### 19.What could go wrong if you use the test set to tune hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18434c6",
   "metadata": {},
   "source": [
    "If you use the test set to tune hyperparameters, you run the risk of overfitting the hyperparameters to the test data, which can lead to a model that performs well on the test data but poorly on new, unseen data.\n",
    "\n",
    "The test set should be used only once, after the model has been trained and the hyperparameters have been tuned on the validation set. The purpose of the test set is to evaluate the final performance of the model on new, unseen data, and to get an estimate of how well the model is likely to perform in the real world.\n",
    "\n",
    "If you use the test set to tune hyperparameters, you are essentially \"peeking\" at the test data and using it to guide the training process, which can lead to overfitting. This is sometimes referred to as \"data leakage,\" and it can result in a model that performs well on the test data but poorly on new, unseen data.\n",
    "\n",
    "To avoid this problem, it is important to split the data into three parts: the training set, the validation set, and the test set. The training set is used to train the model, the validation set is used to tune the hyperparameters, and the test set is used to evaluate the final performance of the model on new, unseen data. By keeping the test set separate from the training and validation sets, you can get a more accurate estimate of the model's performance on new, unseen data and avoid overfitting to the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c2f27",
   "metadata": {},
   "source": [
    "#### Ratio of train, test, validation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affb2e7",
   "metadata": {},
   "source": [
    "The ratio of train, test, and validation data sets can vary depending on the size of the overall data set, the complexity of the model, and the available computing resources.\n",
    "\n",
    "A common split is to use 60% of the data for the training set, 20% for the validation set, and 20% for the test set. However, this split is not fixed, and some practitioners may choose to use different ratios depending on the specific requirements of the problem.\n",
    "\n",
    "In cases where the data set is very large, it may be possible to use a smaller percentage of the data for the validation and test sets while still getting a reliable estimate of the model's performance. In cases where the model is very simple, it may be possible to use a larger percentage of the data for the training set and a smaller percentage for the validation and test sets.\n",
    "\n",
    "Ultimately, the ratio of train, test, and validation data sets should be chosen based on the specific requirements of the problem and the available resources, with the goal of obtaining a reliable estimate of the model's performance on new, unseen data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
